{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0567734",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [import necessary libraries](#0) \n",
    "\n",
    "-  feature extraction and MRC analysis\n",
    "\n",
    "- \n",
    "  [1 - feature_extraction(class)](#1)\n",
    "   !! includes MRC analysis\n",
    "\n",
    "  \n",
    "    calculates maximum results of parameters\n",
    "    usage example: \n",
    "    fe = feature_extraction(stripped = stripped_nifti_file_path, original = original_nifti_file_path, \n",
    "                                 min_power = 1, max_power = 10, min_step = 1, max_step = 10, \n",
    "                                 ROI_shape = 'circle', \n",
    "                                primary_rate = 'all', secondary_rate = 'all',\n",
    "                                all_powers = True, all_steps = True)\n",
    "    fe.calculate_features(calculate_MRC = True)\n",
    "    MRC_results = fe.MRC_results\n",
    "    \n",
    "    \n",
    "- [2. additional functions for MRC analysis](#2)\n",
    "    -    [2.1 take_rate_relation_list (function)](#2.1)\n",
    "        gives a list which is spesify relationship for calculating rates    \n",
    "    -    [2.2 take_pixel_numbers_of_kernel(function)](#2.2)\n",
    "        gives pixel numbers within spesific ROI\n",
    "    -    [2.3 reshape_arr(d\\function)](#2.3)\n",
    "        this function reshapes given array, \n",
    "        the function change axis by taking the smallest one to the beginning\n",
    "        protect other axis order\n",
    "    -    [2.3.1 reshape_arr_and_axis (function)](#2.3.1)\n",
    "        the function is reshaping array and also giving new directions\n",
    "    -    [2.4 rescale(function)](#2.4)\n",
    "        rescale given array to the [0, 1] range\n",
    "    -    [2.5 take_positive_minimum(function)](#2.5)\n",
    "        gives positive minimum value of array, not zero, not nan\n",
    "    -    [2.6 make_sequence(function)](#2.6)\n",
    "        makes list of numbers \n",
    "    -    [2.7 take_csv_file_path(function)](#2.7)\n",
    "        gives csf file for saving MRS results\n",
    "    -    [2.8 update_special_file_list(function)](#2.8)\n",
    "        checking if the given files proccessed or not \n",
    "    -    [2.9 take_collective_case_infos(function)](#2.9)\n",
    "        gives cases info within collective calculations with plural planes of same sequences\n",
    "    -    [2.10 take_pkl_file_path(function)](#2.10)\n",
    "        gives specific pkl file path\n",
    "    -    [2.11 check_if_MRC_completed(function)](#2.11)\n",
    "        checks if MRC proccess completed \n",
    "    -    [2.12 save_results_to_csv(function)](#2.12)\n",
    "        saves the data as csv file into output folder\n",
    "    -    [2.13 run_apply_async_multiprocessing(function)](#2.13)\n",
    "        run multiprocessing\n",
    "      \n",
    "- [3. EPE Analysis (class)](#3)\n",
    "    making EPE analysis which explained in the article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcfdbef",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "###  import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77473f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "from sys import exit\n",
    "\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('fork') \n",
    "# multiprocessing.set_start_method('spawn') \n",
    "from multiprocessing.pool import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4cefff",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "### 1 - feature extraction and MRC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "366b6cb1-a100-40bc-a4d5-fdf8623a2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_2_df(df1, df2):\n",
    "    \n",
    "    if not df1.empty and not df2.empty:\n",
    "        result_df = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "    elif not df1.empty:\n",
    "        result_df = df1.copy()\n",
    "    elif not df2.empty:\n",
    "        result_df = df2.copy()\n",
    "    else:\n",
    "        result_df = pd.DataFrame()\n",
    "    return(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c8f9e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feature_Extraction():\n",
    "    \n",
    "    \"\"\"\n",
    "    This class is used for calculating the features based on given parameters and \n",
    "        also for calculating the maximum results of each feature\n",
    "        \n",
    "    -- features are created based on specific functions and limits which explained in the article\n",
    "    \n",
    "    params:\n",
    "    *# : explained in the article\n",
    "    stripped (type : str (path_like)) :   stripped nifti file paths  \n",
    "    original (type : str (path_like)) :   original nifti file path\n",
    "    power (type : int)(*#) : power value for calculation power of primary rates \n",
    "    step  (type : int)(*#) : n value for calculation of kernel size \n",
    "    ROI_shape (type : str)(*#) : 'circle' or square : kernel(ROI) shape  \n",
    "    primary_rate (type : str or int)(*#) :    \n",
    "            'all' or value(1<= value <=3) \n",
    "            if primary_rate = 'all' - > all primary rates will be calculated\n",
    "            if primary_rate = int. - > only corresponding primary rate will be calculated \n",
    "    secondary_rate (type : str or int)(*#) : 'all' or value(1<= value <=8) or value list\n",
    "            if secondary_rate = 'all' - > all primary rates will be calculated\n",
    "            if secondary_rate = int. - > only corresponding secondary rate will be calculated \n",
    "    \n",
    "    all_powers (type : bool) : True: each power will be calculated, False: only given power will be calculated\n",
    "    all_steps  (type : bool) : \n",
    "            True:  ROI will be calculated for each step,\n",
    "            False: ROI will be calculated for only given step \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, stripped = 'path', original = 'path', \n",
    "                            min_power = 0, max_power = 0, power = 0,\n",
    "                             min_step = 0, max_step = 0, step = 0,\n",
    "                             ROI_shape = \"square\", \n",
    "                              primary_rate = 'all', secondary_rate = 'all',\n",
    "                                 all_powers = False, all_steps = False):\n",
    "        \n",
    "        ## check given files if in nifti format\n",
    "        self.check_if_file_nifti(stripped)\n",
    "        self.check_if_file_nifti(original)\n",
    "        \n",
    "        ## assign inputs to objects for usage in class\n",
    "        self.stripped_path = stripped\n",
    "        self.original_path = original\n",
    "        self.ROI_shape = ROI_shape\n",
    "        self.primary_rate = primary_rate\n",
    "        self.secondary_rate = secondary_rate\n",
    "        self.power = power\n",
    "        self.secondary_rate_list = []\n",
    "        \n",
    "        ## update secondary rate objects based on given input\n",
    "        if type(secondary_rate) == list:\n",
    "            self.secondary_rate_list = secondary_rate\n",
    "            self.secondary_rate = max(secondary_rate)\n",
    "         \n",
    "        ##list specify final rates denominators (*#)\n",
    "        self.sign_list = take_rate_relation_list() \n",
    "                       \n",
    "        ## creating list for specify step and power limits  \n",
    "        if all_powers:\n",
    "            self.power_sq = make_sequence(max_power, min_power)\n",
    "            \n",
    "        if all_steps:\n",
    "            self.step_sq = make_sequence(max_step, min_step)\n",
    "        else:\n",
    "            self.step_sq = [step]\n",
    "            \n",
    "        \n",
    "        ## specify column names for storage MRC results\n",
    "        self.columns = ['shape', 'primary_rate', 'secondary_rate', 'step', 'power', 'MRC_value']\n",
    "        \n",
    "    \n",
    "    def calculate_features(self, calculate_MRC = False):\n",
    "        \n",
    "            \n",
    "        \"\"\"\n",
    "        main function of the class\n",
    "        this function is called for calculating MRC results\n",
    "\n",
    "        output: \n",
    "        all_MRC_results (type : dictionary) : MRC results, keys : steps\n",
    "         \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.calculate_MRC = calculate_MRC\n",
    "\n",
    "        ####------- 1. loading data        \n",
    "        strp_img = nib.load(self.stripped_path) ## loading stripped image \n",
    "        org_img = nib.load(self.original_path)  ## loading original image\n",
    "        \n",
    "        # getting arrays\n",
    "        self.strp_data = strp_img.get_fdata() ## loading stripped image pixel values\n",
    "        self.org_data = org_img.get_fdata()   ## loading original image pixel values\n",
    "\n",
    "        ####-------- 2. Preprocessing of the data (*#)\n",
    "        self.data = self.preprocessing_data()\n",
    "\n",
    "         \n",
    "        ####-------- 3. Processing of the Data (calculating MRC for per pixel and per feature) (*#)\n",
    "                \n",
    "        ## preparing denominators for primary rates\n",
    "        ## i_0(AO1), i_MAD(AO2), i_MdAD(AO3)  (*#)\n",
    "        self.core_denoms = self.take_denominators()\n",
    "                                        \n",
    "        ### start MRC dataframe for storage maximum of rates results for per feature if desired\n",
    "        if self.calculate_MRC:\n",
    "            self.MRC_results = pd.DataFrame(columns = self.columns)  \n",
    "             \n",
    "        for step_size in self.step_sq:\n",
    "  \n",
    "            ##---- 3.1 preparation of array for processing\n",
    "            # taking pixel number of ROI for calculations\n",
    "            self.kernel_pix_num = take_pixel_numbers_of_kernel(kernel_len = 2 * step_size + 1,\n",
    "                                                               shape = self.ROI_shape)\n",
    "            # creating 4 dimensional cluster for making calculations fastly\n",
    "            self.cluster = self.make_kernel_cluster(step = step_size)\n",
    "            \n",
    "            ### object for storage each rates\n",
    "            self.rates = {}    \n",
    "\n",
    "            ##---- 3.2 calculating standart deviations (σ_all, σ_nbh) for primary rates\n",
    "            self.std_all, self.std_ngb = self.std_calculation()\n",
    "\n",
    "            ##----- 3.3 calculating difference between standart deviations : \"σ_all- σ_nbh\"\n",
    "            self.std_diff = self.std_all - self.std_ngb\n",
    "            \n",
    "            ## 3.3.0- the pixel whose \"σ_all- σ_nbh\" < 0 means their harmony is better with together\n",
    "            ## means: we don't need to calculate them             \n",
    "            self.std_diff[self.std_diff <= 0] = np.nan\n",
    "\n",
    "            ##------3.4 calculation MRC results of primary rates (rates without addditional operation) \n",
    "            self.rates['1'] = self.primary_rates_calculation()\n",
    "\n",
    "            ## calculate MRC results for first rate            \n",
    "            if self.calculate_MRC:\n",
    "                self.calculate_MRC_results(step_size =step_size, rate_id = 1)\n",
    "\n",
    "            ##------3.5 calculation MRC results of remained rates \n",
    "            ## limiting rates calculation based on input parameters           \n",
    "            rates_lim = self.find_secondary_rates_limit()\n",
    "\n",
    "            ## calculate rates and obtain maximums (MRC) (*#)    \n",
    "            for j in range (2, rates_lim):\n",
    "                self.rates[str(j)] =  self.rates_calculation(operation_number = j)\n",
    "                ## calculate MRC results for other rates             \n",
    "                if self.calculate_MRC:          \n",
    "                    self.calculate_MRC_results(step_size =step_size, rate_id = j)\n",
    "\n",
    "\n",
    "    def take_single_feature_list(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function gives each pixels feature results in a list fromat without nan values\n",
    "        - the function is usable for single feture calculations\n",
    "        params:\n",
    "        -\n",
    "        (ufunciton use self.rates)\n",
    "        \n",
    "        outputs:\n",
    "        feature_list(type: list) : contains features values from each valid pixel for proccess\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## since we calculated only 1 features, results will return in '1' key\n",
    "        ## since we have only 1 primary rate and power, we will take 0.th axis        \n",
    "        feature_arr = self.rates['1'][0][0].flatten()  \n",
    "        ## remove nans fro taking results only from brain\n",
    "        feature_list = feature_arr[np.logical_not(np.isnan(feature_arr))]\n",
    "        \n",
    "        return(feature_list)\n",
    "\n",
    "                \n",
    "\n",
    "    def calculate_MRC_results(self, step_size = 1, rate_id = 1):\n",
    "\n",
    "        \"\"\"\n",
    "        This function call \"take_MRC_results_df\" function for calculation of MRC results \n",
    "        and store results to \"self.MRC_results\" dataframe \n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if rate_id == 1:\n",
    "\n",
    "            ## take MRC values and features dataframe for 1st secondary rate and all powers\n",
    "            MRC_df = self.take_MRC_results_df(step_size, rate_id)\n",
    "\n",
    "            ## adding to the main result dataframe\n",
    "            self.MRC_results = concat_2_df(self.MRC_results, MRC_df)\n",
    "\n",
    "        else:\n",
    "\n",
    "            ## take MRC values and features dataframe for remained secondary rate and all powers\n",
    "            MRC_df = self.take_MRC_results_df(step_size, rate_id)\n",
    "\n",
    "            ## adding to the main result dataframe\n",
    "            self.MRC_results = concat_2_df(self.MRC_results, MRC_df)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    #### function for the Preprocessing of the data\n",
    "    def preprocessing_data(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function make preprocess; prepare data to the process (*#)\n",
    "        makes:\n",
    "        1- reshaping : put 2 dimensional slice at the 1th and 2nd axis for calculations\n",
    "                       img slices would be like : img[i, :, :] (i indicates slice number)\n",
    "        2- rescaling (*#)\n",
    "        \n",
    "        3- maks non-brain pixels with 'nan'\n",
    "\n",
    "        output: \n",
    "        data (type : array_like) : 3 dimensional array \n",
    "        \"\"\"\n",
    "        \n",
    "        ##calculate min and max values of original image for rescaling\n",
    "        min_org, max_org = np.nanmin(self.org_data), np.nanmax(self.org_data)\n",
    "        \n",
    "        ##  reshaping data to obtain spesific format for calculations \n",
    "        data, self.eliminated_coordinates_number = reshape_arr(self.strp_data) \n",
    "        \n",
    "        ##  rescaling data to obtain spesific format for calculations \n",
    "        data = rescale(data, min_org, max_org)\n",
    "        \n",
    "        ## making 0 values 'nan' for discriminate non-brain values in further calculations\n",
    "        ## note: 0 values represents non-brain pixels\n",
    "        data[data == 0] = np.nan\n",
    "        \n",
    "        # deleting original image and data we will not use original data anymore\n",
    "        del self.org_data, self.strp_data\n",
    "        \n",
    "        return(data)\n",
    "            \n",
    "    def find_secondary_rates_limit(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function gives a number which is indicates limit for calculation final rates\n",
    "\n",
    "        output: \n",
    "        rates_lim (type : int) : secondary rate limit\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.secondary_rate == 'all' :\n",
    "                rates_lim = 9 \n",
    "        else:\n",
    "            rates_lim = self.secondary_rate + 1\n",
    "        return(rates_lim)\n",
    "    \n",
    "    def primary_rates_calculation(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function calculate primary rates with desired powers  (*#)\n",
    "        \n",
    "  \n",
    "        output: \n",
    "        p_rates (type : array_like) : 5 dimensional array which include primary rates with different powers\n",
    "                    0 th axis: indicate primary rate (PR1, PPR2 or PR 3) \n",
    "                    1 th axis: indicate power of primary rate\n",
    "                    3-4-5 : 3 Dimensional data  (primary rates with power)\n",
    "        \"\"\"\n",
    "        \n",
    "        shape = self.data.shape\n",
    "        if self.primary_rate == 'all':\n",
    "            p_rates = np.zeros((3, len(self.power_sq), shape[0], shape[1], shape[2]))  \n",
    "            \n",
    "            for i, p in enumerate(self.power_sq): \n",
    "                \n",
    "                pw_std_all = np.power(self.std_all, p)\n",
    "                pw_std_diff = np.power(self.std_diff, p) \n",
    "                p_rates[0][i] = np.divide(pw_std_all, self.std_ngb, out=np.zeros_like(pw_std_all), where=self.std_ngb!=0)      \n",
    "                p_rates[1][i] = np.divide(pw_std_diff, self.std_all, out=np.zeros_like(pw_std_diff), where=self.std_all!=0)        \n",
    "                p_rates[2][i] = np.divide(pw_std_diff, self.std_ngb, out=np.zeros_like(pw_std_diff), where=self.std_ngb!=0)\n",
    "            \n",
    "            del self.std_all, self.std_ngb  ## we don't need anymore to this objects\n",
    " \n",
    "            return(p_rates)\n",
    "        \n",
    "        ## this part will be processed if primary_rate != 'all': for only choosen primary rate\n",
    "        p_rates = np.zeros((shape[0], shape[1], shape[2])) \n",
    "        \n",
    "\n",
    "        pw_std_all = np.power(self.std_all, self.power)\n",
    "        pw_std_diff = np.power(self.std_diff, self.power) \n",
    "\n",
    "        if self.primary_rate == 1:\n",
    "            p_rates = np.divide(pw_std_all, self.std_ngb, out=np.zeros_like(pw_std_all), where=self.std_ngb!=0) \n",
    "        elif self.primary_rate == 2:\n",
    "            p_rates = np.divide(pw_std_diff, self.std_all, out=np.zeros_like(pw_std_diff), where=self.std_all!=0)    \n",
    "        elif self.primary_rate == 3:    \n",
    "            p_rates = np.divide(pw_std_diff, self.std_ngb, out=np.zeros_like(pw_std_diff), where=self.std_ngb!=0)\n",
    "        else:\n",
    "            print(f\"you entered wrong argument for primary rate\")\n",
    "                \n",
    "        del self.std_all, self.std_ngb  ## we don't need anymore to this objects\n",
    "        \n",
    "        return(p_rates)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def take_MRC_results_df(self, step_size = 1,  rate_id = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function gives MRC results withing a dataframe which contain also features informations\n",
    "\n",
    "        parameter:\n",
    "            rate_id (type: int) : indicates secondary rate id\n",
    "        outputs:\n",
    "        MRC_df (type : pandas_DataFrame) : consist of MRC results \n",
    "                and feature informations based on self.columns\n",
    "                \n",
    "        self.columns = ['shape', 'primary_rate', 'secondary_rate', 'step', 'power', 'MRC_value']\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        ## calculating MRC by takign maximum values\n",
    "        \n",
    "        MRC_info = []\n",
    "                \n",
    "        if self.primary_rate == 'all' :            \n",
    "            MRC_values = np.nanmax(self.rates[str(rate_id)], axis=(2,3,4), initial=0)        \n",
    "            for i, p in enumerate(self.power_sq):                 \n",
    "                MRC_info.append([self.ROI_shape, 1, rate_id, step_size, p, MRC_values[0][i]])\n",
    "                MRC_info.append([self.ROI_shape, 2, rate_id, step_size, p, MRC_values[1][i]])\n",
    "                MRC_info.append([self.ROI_shape, 3, rate_id, step_size, p, MRC_values[2][i]])        \n",
    "            MRC_df = pd.DataFrame(MRC_info, columns = self.columns)\n",
    "            return(MRC_df)\n",
    "        \n",
    "        else:            \n",
    "            MRC_values = np.nanmax(self.rates[str(rate_id)], initial=0)            \n",
    "            for i, p in enumerate(self.power_sq):                 \n",
    "                MRC_info.append([self.ROI_shape, self.primary_rate, rate_id, step_size, p, MRC_values])                \n",
    "            MRC_df = pd.DataFrame(MRC_info, columns = self.columns)                     \n",
    "            return(MRC_df)\n",
    "            \n",
    "        \n",
    "        \n",
    "            \n",
    "    def rates_calculation(self, operation_number = 0):\n",
    "       \n",
    "        \"\"\"\n",
    "        this function gives rates (final rates) results for obtaining MRC values \n",
    "        by using primary rates and denominators  (*#)\n",
    "        \n",
    "        params:\n",
    "        operation_number(type : int) : this number is used for \n",
    "                                        identify which denominator and which primary rate will be used (*#)\n",
    "\n",
    "        output: \n",
    "        s_rates (type : array_like) : 5 dimensional array\n",
    "                        0 th axis: indicate primary rate (PR1, PPR2 or PR 3) \n",
    "                        1 th axis: indicate power of primary rate\n",
    "                        3-4-5 : 3 Dimensional data  (final rates)\n",
    "                        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.clean_data(operation_number - 1 ) ## cleaning data which are not necessary anymore\n",
    "        \n",
    "        # s0 same with operation_number, s1: primary rate id, s2: denominator id\n",
    "        s0, s1, s2 = self.sign_list[operation_number - 2] \n",
    "        shape = self.data.shape\n",
    "\n",
    "        \n",
    "        if self.primary_rate == 'all':\n",
    "            s_rates = np.zeros((3, len(self.power_sq), shape[0], shape[1], shape[2]))               \n",
    "            ## setting denominator based on data shape\n",
    "            denom_4 = self.expand_3dim_array_to_4dim(self.core_denoms[s2], len(self.power_sq)) \n",
    "            denom_5 = self.expand_4dim_array_to_5dim(denom_4, 3)             \n",
    "            ## calculating rate result by dividing primary rate(with appropriate powere) to denominator\n",
    "            s_rates =  np.divide(self.rates[s1], denom_5, out=np.zeros_like(self.rates[s1]), where = denom_5!=0)     \n",
    "            return(s_rates)\n",
    "        \n",
    "        ## this part will be processed if primary_rate != 'all':        \n",
    "        s_rates = np.zeros((shape[0], shape[1], shape[2]))         \n",
    "        denom = self.core_denoms[s2]         \n",
    "        s_rates =  np.divide(self.rates[s1], denom, out=np.zeros_like(self.rates[s1]), where=denom!=0)\n",
    "        return(s_rates)\n",
    "        \n",
    "\n",
    "    def std_calculation(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        this function gives standard deviations of ROI for each pixel located at the center of the ROI (*#)\n",
    "        \n",
    "\n",
    "        output: \n",
    "        std_all (type : array_like) : standard deviation of each pixel within ROI (σ_all)\n",
    "        std_ngb (type : array_like) : standard deviation of only neighbor pixels within ROI (σ_nbh)\n",
    "                                      (σ_nbh : center pixel is not included)\n",
    "                        \n",
    "        \"\"\"\n",
    "                \n",
    "        # calculation standart deviations (σ_all )throughout ROI(kernel)             \n",
    "        std_all = np.std(self.cluster, axis = 1)      \n",
    "        \n",
    "        # for calculating  σ_nbh:         \n",
    "        # calculate index of center pixel\n",
    "        center_index = int ((self.kernel_pix_num - 1) / 2 )        \n",
    "        # remove center pixel from cluster\n",
    "        sq_ind = int((self.kernel_pix_num -1) / 2)        \n",
    "        self.cluster = np.delete(self.cluster, sq_ind , axis = 1)       \n",
    "        ### calculate naighbors standard deviations (σ_nbh )\n",
    "        std_ngb = np.std(self.cluster, axis = 1)       \n",
    "        del self.cluster ## we don't need cluster anymore        \n",
    "        return(std_all, std_ngb)\n",
    "      \n",
    "        \n",
    "\n",
    "            \n",
    "    \n",
    "    ### creating 4 dimensional array for further calculations   \n",
    "    def make_kernel_cluster(self, step = 0):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        this function created a cluster which is 4 dimensional, \n",
    "        first dimension same with pixel number in the ROI\n",
    "        the cluster were used for making ROI calculations fastly, \n",
    "        at the 1th axis, each pixel ROI members were storage, \n",
    "        and the subject pixel is localized at the center\n",
    "        \n",
    "        params:\n",
    "        step(type : int) (*#): n : specify kernel size ( = (kernel size - 1)) / 2 )\n",
    "\n",
    "        output: \n",
    "        cluster (type : array_like) : 4 dimensional array\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # creating initial arrays for procccess\n",
    "        ## padding data for further calculations\n",
    "        data_shape = self.data.shape\n",
    "        kernel_len = step * 2 + 1\n",
    "        pad_data = np.zeros((data_shape[0], kernel_len + 1 + data_shape[1],  kernel_len + 1 + data_shape[2]))\n",
    "        pad_data[:, step+1 : -step-1,  step+1 : -step-1] = self.data\n",
    "        \n",
    "        ## 4- dimensional cluster will include all neighbors pixels in 1th axis and center pixels will be at the center        \n",
    "        img_shape = self.data.shape\n",
    "        cluster = np.zeros((img_shape[0], self.kernel_pix_num ,img_shape[1], img_shape[2]))     \n",
    "        \n",
    "        ## defining some limits for carrying data to padded data while matching neighbors with the same axis of the center pixel       \n",
    "        if self.ROI_shape == \"square\":\n",
    "            coordinates_dif = [(x,y) for x in range(-step, step+1) for y in range(-step, step+1)]\n",
    "            coordinates_limits = [([step+x+1, -step+x-1], [step+y+1, -step+y-1]) for x,y in coordinates_dif]\n",
    "            for i, limit in enumerate (coordinates_limits):            \n",
    "                cluster[:, i, :, :] = pad_data[:, limit[0][0] : limit[0][1],  limit[1][0] : limit[1][1]]\n",
    "            return(cluster)\n",
    "        \n",
    "        elif self.ROI_shape == \"circle\":\n",
    "            coordinates_dif = [(x,y) for x in range(-step, step+1) for y in range(-step, step+1) if ( x**2 + y**2 ) <= step**2]\n",
    "            coordinates_limits = [([step+x+1, -step+x-1], [step+y+1, -step+y-1]) for x,y in coordinates_dif]\n",
    "            for i, limit in enumerate (coordinates_limits):            \n",
    "                cluster[:, i, :, :] = pad_data[:, limit[0][0] : limit[0][1],  limit[1][0] : limit[1][1]]\n",
    "            return(cluster)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    ### preparing denomitors for additional operations \n",
    "    \n",
    "    def take_denominators(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function creates 3 core denominator for calculating secondary rates (*#)\n",
    "\n",
    "        output: \n",
    "        denom (type : array_like) : 3 dimensional array\n",
    "        \"\"\"\n",
    "\n",
    "        denom = {}\n",
    "        \n",
    "        ## MAD, MdAD  calculation            \n",
    "        median_value = np.nanmedian(self.data)    \n",
    "        midrange_value = (np.nanmax(self.data) + np.nanmin(self.data)) / 2\n",
    "        \n",
    "        ## this denominator for intensity:  \" i_0(AO1) \" (*#)\n",
    "        denom['0'] = np.copy(self.data)\n",
    "        \n",
    "        ## this denominator for median absolute deviation:  \"i_MAD(AO2) \" (*#)\n",
    "        denom['1'] = np.abs(self.data - median_value)\n",
    "        denom['1'][denom['1'] == 0] = take_positive_minimum(denom['1'])\n",
    "               \n",
    "        ## this denominator for midrange absolute deviation:  \"i_MdAD(AO3) \"  (*#)\n",
    "        denom['2'] = np.abs(self.data - midrange_value)        \n",
    "        denom['2'][denom['2'] == 0] = take_positive_minimum(denom['2'])\n",
    "        \n",
    "        return(denom)\n",
    "\n",
    " \n",
    "\n",
    "        \n",
    "    def clean_data(self, operation_number):    \n",
    "        \n",
    "        \"\"\"\n",
    "        this function deletes un-necessary objects for reduce memory           \n",
    "        \"\"\"       \n",
    "        if operation_number == 4:\n",
    "            if 4 not in self.secondary_rate_list:\n",
    "                del self.rates['4']\n",
    "        elif operation_number == 5 :\n",
    "            if 1 not in self.secondary_rate_list:\n",
    "                del self.rates['1']\n",
    "        elif operation_number == 6 :\n",
    "            if 6 not in self.secondary_rate_list:\n",
    "                del self.rates['6']\n",
    "        elif operation_number == 7 :\n",
    "            if 2 not in self.secondary_rate_list:\n",
    "                del self.rates['2']\n",
    "            if 7 not in self.secondary_rate_list:\n",
    "                del self.rates['7']\n",
    "        elif operation_number == 8 :\n",
    "            if 3 not in self.secondary_rate_list:\n",
    "                del self.rates['3']\n",
    "     \n",
    "    def expand_3dim_array_to_4dim(self, array, added_size):\n",
    "        \"\"\"\n",
    "        this function make 3 dimensional array 4 dimensional by repeating 3 dimensional array\n",
    "        --the additional axis is added to the beginning\n",
    "        --aim : making calculations in 4 dimension without using for loops\n",
    "        \n",
    "        output: \n",
    "        array(type: array_like) : 4 dimensonal array\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(array.shape) == 3:        \n",
    "            array = np.expand_dims(array, axis=0)\n",
    "            array = np.tile(array,(added_size,1,1,1))\n",
    "            return(array)\n",
    "        return(print(f\"given array is not 3 dimensional\"))\n",
    "    \n",
    "    def expand_4dim_array_to_5dim(self, array, added_size):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function make 4 dimensional array 5 dimensional by repeating 4 dimensional array\n",
    "        --the additional axis is added to the beginning\n",
    "        --aim : making calculations in 5 dimension without using for loops\n",
    "        \n",
    "        output: \n",
    "        array(type: array_like) : 5 dimensonal array\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(array.shape) == 4:        \n",
    "            array = np.expand_dims(array, axis=0)\n",
    "            array = np.tile(array,(added_size,1,1,1,1))\n",
    "            return(array)\n",
    "        \n",
    "        return(print(f\"given array is not 4 dimensional\"))\n",
    "            \n",
    "        \n",
    "    ### control function for if nifti files paths showing if nifti files      \n",
    "    def check_if_file_nifti(self, file_path):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function check if file is nifti \n",
    "        \n",
    "        params:\n",
    "        file_path (type: string(path_like)) : file path\n",
    "        \n",
    "        output: \n",
    "        \n",
    "        if file path is not showing nifti file, error message will be shown and\n",
    "        proccess wil be ended with exit command\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if_file = os.path.isfile(file_path)\n",
    "        \n",
    "        if '.nii.gz' not in file_path:\n",
    "            print('the path indicated nifti files extension is not \".nii.gz\"')\n",
    "            if_file = False\n",
    "            print(f' \"{file_path}\" path is not showing a nifti file')\n",
    "            print(f\"please check nifti files within the given path\")\n",
    "            exit()\n",
    "            \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4a658",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "### 2. additional functions for MRC analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ceb53f",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "#### 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c87bfe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_rate_relation_list():\n",
    "    \"\"\"\n",
    "    this function gives a list which is\n",
    "    specify primary, additional operations (denominators) and secondary rates relations\n",
    "    example:  ['2', '1', '0']: \n",
    "    '2' : Secondary Rate 2 will be calculated with follows: \n",
    "    '1' : gives secondary rate and this rate will be divided to\n",
    "        following denominator for obtain Secondary Rate 2\n",
    "    '0' : core denominator 0\n",
    "    \n",
    "    in the other words: \n",
    "    Secondary Rate 2  = Secondary Rate 1 / core denominator 0\n",
    "    \n",
    "    output:\n",
    "    rates(type: list)\n",
    "        \n",
    "    \"\"\"\n",
    "    rates = []\n",
    "    rates.append(['2', '1', '0']) # taking serie_2 by calculating first rates by dividing intensities\n",
    "    rates.append(['3', '1', '1']) # taking serie_3 by calculating first rates by dividing distance to median\n",
    "    rates.append(['4', '1', '2']) # taking serie_4 by calculating first rates by dividing distance to midrange\n",
    "    rates.append(['5', '2', '1']) # taking serie_5 by calculating first rates by dividing intensity and distance to median\n",
    "    rates.append(['6', '2', '2']) # taking serie_6 by calculating first rates by dividing intensity and distance to midrange\n",
    "    rates.append(['7', '3', '2']) # taking serie_7 by calculating first rates by dividing distance to midrange and median\n",
    "    rates.append(['8', '5', '2']) # taking serie_8 by calculating first rates by dividing intensity and distance to midrange and median\n",
    "    return(rates)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abaaeb3",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5fbca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_pixel_numbers_of_kernel(kernel_len, shape = \"square\"):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives pixel number within spesific ROI\n",
    "    parameters:\n",
    "    kernel_len(type: int) : ROI size:\n",
    "        if shape      ==  \"square\"   : kernel_len = edge pixel number\n",
    "        if shape == \"circle\"         : kernel_len = pixel number on diameter\n",
    "     \n",
    "    shape(type: str)  : ROI shape\n",
    "    outputs:\n",
    "    pixel_number(type:int) : pixel number within spesific ROI\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if shape == \"square\" : \n",
    "        pixel_number = np.square(kernel_len)\n",
    "        return(int(pixel_number))\n",
    "    if shape == \"circle\":\n",
    "        ## take radius size for circle ROI\n",
    "        r = int((kernel_len - 1) / 2 )\n",
    "        pixel_number = np.sum([1 for i in range (-r, r+1) for j in range (-r, r+1) if ( i**2 + j**2 ) <= r**2])\n",
    "        return(int(pixel_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea9f6d",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "#### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f0f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_arr(img_arr):\n",
    "    \"\"\"\n",
    "    this function reshapes given array, the function change axis by taking the smallest one to the beginning\n",
    "    -- protect other axis order\n",
    "    \n",
    "    parameters:\n",
    "    img_arr(type: numpy_array) : 3 dimensional numpy array\n",
    "\n",
    "    outputs:\n",
    "    new_arr(type: numpy_array) : 3 dimensional numpy array\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "    img_shape = np.array(img_arr.shape)\n",
    "    x, y, z = img_shape[0], img_shape[1], img_shape[2]\n",
    "\n",
    "       \n",
    "    min_slice = np.argmin(img_shape)\n",
    "\n",
    "\n",
    "    img_shape[0] = img_shape[min_slice]\n",
    "    img_shape[min_slice] = x\n",
    "    if min_slice == 2:\n",
    "        img_shape[1], img_shape[2] = x, y\n",
    "    new_arr = np.zeros((img_shape[0], img_shape[1], img_shape[2]))\n",
    "    for i in range (0, img_shape[0]):\n",
    "        if min_slice == 0:\n",
    "           new_arr[i, :, :] =  img_arr[i, :, :]\n",
    "        elif min_slice == 1:\n",
    "           new_arr[i, :, :] =  img_arr[:, i, :]\n",
    "        else:\n",
    "           new_arr[i, :, :] =  img_arr[:, :, i]\n",
    "  \n",
    "    \n",
    "    arr_shape = new_arr.shape\n",
    "    min0 = arr_shape[0]\n",
    "    min1 = arr_shape[1]\n",
    "    min2 = arr_shape[2]\n",
    "    max0 = 0\n",
    "    max1 = 0\n",
    "    max2 = 0\n",
    "    \n",
    "    arr_0 = np.zeros((arr_shape[1], arr_shape[2]))    \n",
    "    for i in range (0, arr_shape[0]):\n",
    "        min0 = i\n",
    "        if new_arr[i, :, :].any() != arr_0.any() :\n",
    "            break\n",
    "    for i in reversed(range(arr_shape[0])):\n",
    "        max0 = i\n",
    "        if new_arr[i, :, :].any() != arr_0.any() :\n",
    "            break\n",
    "            \n",
    "    arr_1 = np.zeros((arr_shape[0], arr_shape[2]))    \n",
    "    for i in range (0, arr_shape[1]):\n",
    "        min1 = i\n",
    "        if new_arr[:, i, :].any() != arr_1.any() :\n",
    "            break\n",
    "    for i in reversed(range(arr_shape[1])):\n",
    "        max1 = i\n",
    "        if new_arr[:, i, :].any() != arr_1.any() :\n",
    "            break\n",
    "            \n",
    "    arr_2 = np.zeros((arr_shape[0], arr_shape[1]))    \n",
    "    for i in range (0, arr_shape[2]):\n",
    "        min2 = i\n",
    "        if new_arr[:, :, i].any() != arr_2.any() :\n",
    "            break\n",
    "    for i in reversed(range(arr_shape[2])):\n",
    "        max2 = i\n",
    "        if new_arr[:, :, i].any() != arr_2.any() :\n",
    "            break\n",
    "            \n",
    "    new_arr = new_arr[min0:max0+1, min1:max1+1, min2:max2+1]\n",
    "\n",
    "    \n",
    "            \n",
    "    return(new_arr, [min0, min1, min2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370dc04b",
   "metadata": {},
   "source": [
    "<a name='2.3.1'></a>\n",
    "#### 2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c088b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_arr_and_axis(img_arr, axes):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is rotating an array in a specific way. \n",
    "    The function is reshaped array by making the array's 0th axis the smallest length of an array \n",
    "    which is actually for making slicing always at the 0th axis. \n",
    "    \n",
    "    params:\n",
    "    img_arr (type: numpy_array) : 3 dimensional numpy array\n",
    "    axes (type: str) : (1, 3) dimensional array which is giving axis directions, such as ['A', 'S', 'L']\n",
    "    \n",
    "    outputs:\n",
    "    new_arr(type: numpy_array) : 3 dimensional numpy array \n",
    "    new_axes(type: numpy_array) : (1, 3) dimensional array which is giving new axis directions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    img_shape = np.array(img_arr.shape)\n",
    "    x, y, z = img_shape[0], img_shape[1], img_shape[2]\n",
    "    min_slice = np.argmin(img_shape)\n",
    "    img_shape[0] = img_shape[min_slice]\n",
    "    img_shape[min_slice] = x\n",
    "    if min_slice == 2:\n",
    "        img_shape[1], img_shape[2] = x, y\n",
    "    new_arr = np.zeros((img_shape[0], img_shape[1], img_shape[2]))\n",
    "    new_axes = ('m', 'm', 'm')\n",
    "    for i in range (0, img_shape[0]):\n",
    "        if min_slice == 0:\n",
    "            new_arr[i, :, :] =  img_arr[i, :, :]\n",
    "            new_axes = axes\n",
    "        elif min_slice == 1:\n",
    "            new_arr[i, :, :] =  img_arr[:, i, :]\n",
    "            new_axes = (axes[1], axes[0], axes[2])            \n",
    "        else:\n",
    "            new_arr[i, :, :] =  img_arr[:, :, i]\n",
    "            new_axes = (axes[2], axes[0], axes[1])\n",
    "    \n",
    "    return(new_arr, new_axes)\n",
    "\n",
    "\n",
    " \n",
    "def reshape_arr_and_axis_affine(img_arr, axes, original_affine, zooms):\n",
    "    \"\"\"\n",
    "    Reshapes a 3D array and updates the affine matrix to reflect non-isotropic voxel properties.\n",
    "    \n",
    "    params:\n",
    "    img_arr (numpy array): 3D numpy array representing the image data.\n",
    "    axes (tuple): Original axes directions (e.g., ('A', 'S', 'L')).\n",
    "    original_affine (numpy array): Original 4x4 affine matrix of the NIfTI image.\n",
    "    zooms (tuple): Voxel dimensions from the original NIfTI header.\n",
    "    \n",
    "    returns:\n",
    "    new_arr (numpy array): Reshaped 3D numpy array.\n",
    "    new_axes (tuple): New axes directions after reshaping.\n",
    "    new_affine (numpy array): Updated 4x4 affine matrix preserving voxel size and spacing.\n",
    "    \"\"\"\n",
    "    img_shape = np.array(img_arr.shape)\n",
    "    min_slice = np.argmin(img_shape)  # Find the axis with the smallest length\n",
    "\n",
    "    new_arr = img_arr\n",
    "    new_affine = original_affine.copy()\n",
    "    new_axes = axes\n",
    "\n",
    "    if min_slice == 0:  # No change needed if the 0th axis is already the smallest\n",
    "        new_arr = img_arr\n",
    "    elif min_slice == 1:  # Move axis 1 to the front\n",
    "        new_arr = np.transpose(img_arr, (1, 0, 2))\n",
    "        new_affine[:3, :] = new_affine[[1, 0, 2], :]  # Swap rows 0 and 1\n",
    "        # Adjust the voxel size scaling for the entire affine matrix\n",
    "        new_affine[:3, :3] *= np.array([zooms[1], zooms[0], zooms[2]]) / np.array([zooms[0], zooms[1], zooms[2]])\n",
    "        new_axes = (axes[1], axes[0], axes[2])\n",
    "    elif min_slice == 2:  # Move axis 2 to the front\n",
    "        new_arr = np.transpose(img_arr, (2, 0, 1))\n",
    "        new_affine[:3, :] = new_affine[[2, 0, 1], :]  # Swap rows 0 and 2\n",
    "        # Adjust the voxel size scaling for the entire affine matrix\n",
    "        new_affine[:3, :3] *= np.array([zooms[2], zooms[0], zooms[1]]) / np.array([zooms[0], zooms[1], zooms[2]])\n",
    "        new_axes = (axes[2], axes[0], axes[1])\n",
    "\n",
    "    return new_arr, new_axes, new_affine\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def update_affine_for_new_axes(original_affine, min_slice):\n",
    "    \"\"\"\n",
    "    Adjusts the affine matrix based on the new axes order after reshaping.\n",
    "    \n",
    "    params:\n",
    "    original_affine (numpy array): Original 4x4 affine matrix of the NIfTI image.\n",
    "    min_slice (int): Index of the smallest axis used for reshaping.\n",
    "    \n",
    "    returns:\n",
    "    new_affine (numpy array): Modified 4x4 affine matrix.\n",
    "    \"\"\"\n",
    "    new_affine = original_affine.copy()\n",
    "    \n",
    "    # Adjust the affine matrix rows based on the new axes order\n",
    "    if min_slice == 1:  # If the 1st axis becomes the 0th axis\n",
    "        new_affine[:3, :] = new_affine[[1, 0, 2], :]\n",
    "    elif min_slice == 2:  # If the 2nd axis becomes the 0th axis\n",
    "        new_affine[:3, :] = new_affine[[2, 0, 1], :]\n",
    "    \n",
    "    return new_affine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f65094",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "#### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3d203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(data, min_value, max_value):\n",
    "    \n",
    "    \"\"\"\n",
    "    rescaling data to [0, 1] range by given minimum and maximum values\n",
    "    \n",
    "    params:\n",
    "    data(type: numpy_array) : numpy array\n",
    "    min_value(type: float/int) : spesific minimum value\n",
    "    max_value(type: float/int) : spesific maximum value\n",
    "    \n",
    "    outputs:\n",
    "    rescaled_data(type: numpy_array) : rescaled data between to [0, 1]\n",
    " \n",
    "    \"\"\"\n",
    "    rescaled_data = (data - min_value) / (max_value - min_value)\n",
    "    \n",
    "    return (rescaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed1028",
   "metadata": {},
   "source": [
    "<a name='2.5'></a>\n",
    "#### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "550612f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_positive_minimum(data):\n",
    "    \"\"\"\n",
    "    this function is giving a minimum value which is neither nan nor 0\n",
    "    params : \n",
    "    data(type: numpy array) :numpy array\n",
    "    outputs:\n",
    "    minimum_value(type: float/ int) : positive minimum of data , not nan, not 0\n",
    "    \"\"\"\n",
    "    \n",
    "    data[data == 0] = np.nan\n",
    "    minimum_value = np.nanmin(data)\n",
    "    \n",
    "    return(minimum_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c28e83",
   "metadata": {},
   "source": [
    "<a name='2.6'></a>\n",
    "#### 2.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc0ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.6\n",
    "def make_sequence(value_max = 0, value_min = 0):  \n",
    "    \n",
    "    \"\"\"\n",
    "    The function create sequence with a numbers between given params\n",
    "    \n",
    "    params:\n",
    "    value_max(type: int) : maximum number for sequence\n",
    "    value_min(type: int) : minimum number for sequence\n",
    "    \n",
    "    output:\n",
    "    sequence(type: numpy array) : 1 dimensional array\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    if value_min > value_max:\n",
    "        print('INPUT Error :')\n",
    "        print('minimum value were given smaller then maximum value')\n",
    "        return('ERROR')\n",
    "    \n",
    "    else:\n",
    "        sequence = np.arange(value_min, value_max + 1)\n",
    "        return(sequence)\n",
    "                                 \n",
    "    return(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f421f3f2",
   "metadata": {},
   "source": [
    "<a name='2.7'></a>\n",
    "#### 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0756d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.7 \n",
    "    \n",
    "def take_csv_file_path(output_folder_path = '', original_file_path = '',\n",
    "                        original_file_root_path =  ''):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives csv file path for saving results\n",
    "    parameters:\n",
    "    \n",
    "    output_folder_path (type: str(path_like)) : main folder path for saving MRC results \n",
    "    original_file_path (type: str(path_like)) : path of file which are used for MRC calculation\n",
    "    original_file_root_path (type: str(path_like)) : root path of file for spesify \n",
    "                                            child and parent folder for saving csv files\n",
    "                  \n",
    "    output_path (type: str(path_like)) : gives csv path file for saving MRC results \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    child_path = original_file_path.split(original_file_root_path)[-1][1:-7]\n",
    "    output_path = os.path.join(output_folder_path, child_path + '_MRC.csv')\n",
    "    \n",
    "    return(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d96673",
   "metadata": {},
   "source": [
    "<a name='2.8'></a>\n",
    "#### 2.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e54f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_special_file_list(file_names_list, output_path = '', stripped_path =  ''):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function is checking if the given files proccessed or not \n",
    "    \n",
    "    params:\n",
    "    file_names_list (type : list) :file name lists which will be proccessed \n",
    "    output_path(type: str : path-like) : folder path which includes proccessed files \n",
    "    stripped_path (type: str : path-like) : folder path which includes files to be processed \n",
    "    \n",
    "    output:\n",
    "    updated_list : files list which include only not processed files\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    updated_list = []    \n",
    "    for file_names in file_names_list:       \n",
    "        child_path = file_names[0].split(stripped_path)[-1][1:-7]\n",
    "        csv_path = os.path.join(output_path, child_path + '_MRC.csv')\n",
    "        if not os.path.isfile(csv_path):\n",
    "            updated_list.append(file_names)\n",
    "    return(updated_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6744832",
   "metadata": {},
   "source": [
    "<a name='2.9'></a>\n",
    "#### 2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd1b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_collective_case_infos(situations, info_path, sequence_folders):\n",
    "\n",
    "    \"\"\"\n",
    "    function gives cases info within collective calculations with plural planes of same sequences\n",
    "    params:\n",
    "    situations (type : array or list) : array includes situations keywords. example : ['patients', 'controls']\n",
    "    info_path (type : str (path-like)) : excel file path which is consist of case-MRI data information\n",
    "    sequence_folders(type : array or list) : includes sequence folder names. example ['t1', 't2']\n",
    "    \n",
    "    output:\n",
    "    all_case_info (type: pandas dataframe) : includes case id and info and situation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    all_case_info = pd.DataFrame()\n",
    "\n",
    "    for situation in situations:\n",
    "        ## reading info file\n",
    "        file_ = pd.read_excel(info_path, sheet_name = None)\n",
    "        ## taking sheet names\n",
    "        sheet_names = list(file_.keys())\n",
    "        ## filtering sheet names based on situation\n",
    "        sheet_names = [sheet for sheet in sheet_names if situation in sheet]\n",
    "        ## filtering sheet names depend on subfolders which are belong to same sequences\n",
    "        sheet_names = [sheet for sheet in sheet_names for subfolder in sequence_folders if subfolder in sheet]\n",
    "        ##create dataframe for loading infos\n",
    "        info_df = pd.DataFrame(columns = ['input_name'])\n",
    "        for sheet in sheet_names :\n",
    "            df = file_[sheet][['input_name', 'case_name']]\n",
    "            new_column_name = sheet.split(situation + '_')[1]\n",
    "            df = df.rename(columns = {'case_name': new_column_name })\n",
    "            info_df = pd.merge(info_df, df, how = \"outer\", on=['input_name'])\n",
    "            info_df['situation'] = [situation] * len(info_df)\n",
    "            info_df['case'] = ['coll_case' + str(i) for i in range (1, len(info_df) + 1)]\n",
    "        all_case_info = pd.concat([info_df, all_case_info] , axis = 0, ignore_index=True)        \n",
    "    return(all_case_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f284c",
   "metadata": {},
   "source": [
    "<a name='2.10'></a>\n",
    "#### 2.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dacbb219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_pkl_file_path(feature_vals, feature_output_folder):\n",
    "    \"\"\"\n",
    "    gives specific pkl file path\n",
    "    params:\n",
    "    feature_vals(type: array or list) : features of pkl file \n",
    "    feature_output_folder(type: str) : folder path for saving the pkl file into\n",
    "    output:\n",
    "    file_path (type: str (path-like)) : the pkl file path for saving data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    features_list = [str(feat) for feat in feature_vals] + ['.pkl']\n",
    "    file_name = \"-\".join(features_list)\n",
    "    file_path = os.path.join(feature_output_folder, file_name)\n",
    "    return(file_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bfa98a",
   "metadata": {},
   "source": [
    "<a name='2.11'></a>\n",
    "#### 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85be3b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_MRC_completed(output_folder_path, original_file_path,\n",
    "                    original_file_root_path, min_step, max_step):\n",
    "    \n",
    "    \"\"\"\n",
    "    the function checks if MRC proccess completed \n",
    "    \n",
    "    params:\n",
    "    output_folder_path(type : str (path-like)) : output folder path for MRC results\n",
    "    original_file_path(type : str (path-like)) : original file path which MRC will be applied\n",
    "    original_file_root_path(type : str (path-like)) : folder path which includes original files\n",
    "    min_step(type : int) : minimum step for MRC calculation\n",
    "    max_step(type : int) : maximum step for MRC calculation\n",
    "    \n",
    "    output:\n",
    "    MRC_completed (type : bool) : True: MRC completed / False: MRC process not completed\n",
    "    \"\"\"\n",
    "    \n",
    "    MRC_completed = False\n",
    "    csv_file_path = take_csv_file_path(output_folder_path, original_file_path, original_file_root_path)   \n",
    "    is_file = os.path.isfile(csv_file_path)   \n",
    "    if not is_file:       \n",
    "        return(MRC_completed)\n",
    "    else:\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "        data_step_min = data['step'].min()        \n",
    "        if min_step < data_step_min:\n",
    "            return(MRC_completed)      \n",
    "        data_step_max = data['step'].max()      \n",
    "        if max_step > data_step_max:\n",
    "            return(MRC_completed)\n",
    "    MRC_completed = True\n",
    "    return(MRC_completed)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52ade57",
   "metadata": {},
   "source": [
    "<a name='2.12'></a>\n",
    "#### 2.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f3d2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(data, output_folder_path, original_file_path,\n",
    "                    original_file_root_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    saves the data as csv file into output folder\n",
    "    params:\n",
    "    data (type: pandas dataframe)    : pandas dataframe for saving to csv file\n",
    "    output_folder_path(type : str (path-like)) : folder path for saving csv file into\n",
    "    original_file_path(type : str (path-like)) : original file path for creating csv file name\n",
    "    original_file_root_path(type : str (path-like)) : original files folder path for creating csv file name\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    csv_file_path = take_csv_file_path(output_folder_path, original_file_path, original_file_root_path)    \n",
    "    if os.path.isfile(csv_file_path):        \n",
    "        old_data = pd.read_csv(csv_file_path)       \n",
    "        new_data = pd.concat([old_data, data], join='outer', ignore_index=True)     \n",
    "        new_data.to_csv(csv_file_path, index=False, index_label = False)      \n",
    "    else:      \n",
    "        data.to_csv(csv_file_path, index=False, index_label = False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f967d6",
   "metadata": {},
   "source": [
    "<a name='2.13'></a>\n",
    "#### 2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a21a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for making multipl proccesses at same time\n",
    "\n",
    "def run_apply_async_multiprocessing(func, argument_list, num_processes):\n",
    "    \n",
    "    \"\"\"\n",
    "    run multiprocessing \n",
    "    params: \n",
    "    func (type: function): function which will be run\n",
    "    argument_list (type: list): arguments list will be used in a function\n",
    "    num_processes (type: int): number of multipl proccess\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    pool = Pool(processes=num_processes)\n",
    "\n",
    "    jobs = [pool.apply_async(func=func, args=(*argument,)) if isinstance(argument, tuple) else pool.apply_async(func=func, args=(argument,)) for argument in argument_list]\n",
    "    pool.close()\n",
    "    \n",
    "    result_list_tqdm = []\n",
    "    for job in tqdm(jobs):\n",
    "        result_list_tqdm.append(job.get())    \n",
    "    return result_list_tqdm\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f165937",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "\n",
    "## 3. EPE Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59674dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPE_analysis():\n",
    "\n",
    "    \"\"\"   \n",
    "    EPE analysis : Each point evaluation analysis\n",
    "    This class is for making EPE analysis which explained in the article\n",
    "    \n",
    "    params for initialization :\n",
    "    control_id (type: str) : general id word of control cases\n",
    "    patient_id (type: str) : general id word of patients\n",
    "    PCVL (type: str) : \" per case value limit \" / number of results for calculation per case \n",
    "    ** this limitation is used for decreasing coding time and memory \n",
    "    feature_output_folder (type: str (path-like)) : folder path for saving results\n",
    "    follow_proccess (type: bool) :  False : Don't give files names information continually\n",
    "                                    True: give files names information continually\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def __init__(self, control_id, patient_id, PCVL  = 1000,\n",
    "                 feature_output_folder = '', follow_proccess = False):\n",
    "        \n",
    "        self.control_id = control_id\n",
    "        self.patient_id = patient_id\n",
    "        self.feature_output_folder = feature_output_folder\n",
    "        self.follow_proccess = follow_proccess\n",
    "        self.PCVL = PCVL # per case value limit\n",
    "    \n",
    "                     \n",
    "    def   activate_EPE_analysis(self, feature_info, \n",
    "                                controls_file_paths,  patients_file_paths,\n",
    "                                specificity_min = 0.7, sensitivity_min = 0.7,\n",
    "                                activate_collective_analysis = False, \n",
    "                                info_path = '', sequence_folders = []):\n",
    "        \n",
    "        \"\"\"\n",
    "        params:\n",
    "        feature_info (type: pandas data frame) : parameters' features infos \n",
    "        controls_file_paths (type: str (path-like)) : coltrol cases files paths\n",
    "        patients_file_paths (type: str (path-like)) : patients files paths\n",
    "        specificity_min (type: float) : minimum specifity for selecting parameters\n",
    "        sensitivity_min (type: float) : minimum sensitivity for selecting parameters\n",
    "        activate_collective_analysis (type: bool) : True: the calculation is collective, \n",
    "                                                    False: the calculation isnot collective analysis\n",
    "        info_path (type: str (path-like)) : Excel path which is consisting case matching informations\n",
    "                                            This argument necessary for collective analysis\n",
    "        sequence_folders (type: list or array) : sequence folders list \n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        ## specificity info for collective analysis\n",
    "        self.activate_collective_analysis =  activate_collective_analysis     \n",
    "        self.info_path, self.sequence_folders = info_path, sequence_folders\n",
    "\n",
    "        ## specificity F1 limit for analysis\n",
    "        self.specificity_min, self.sensitivity_min = specificity_min, sensitivity_min \n",
    "                        \n",
    "        ## list for record stat results\n",
    "        self.stat_results = []   \n",
    "\n",
    "        ## specificity ROI shape for calculating seperately\n",
    "        ROI_shapes = list(feature_info['shape'].unique())  \n",
    "        \n",
    "        for roi_shape in ROI_shapes:      \n",
    "            \n",
    "            features = feature_info[feature_info['shape'] == roi_shape]         \n",
    "            \n",
    "            ## specificity step sizes\n",
    "            step_sizes = features['step'].unique()\n",
    "            \n",
    "            ## take feature info for per step size:\n",
    "            for step in step_sizes:\n",
    "                \n",
    "                features_s = features[features['step'] == step]\n",
    "                self.extract_features(features_s, roi_shape, controls_file_paths, patients_file_paths)\n",
    "                \n",
    "                ## run stat analysis                \n",
    "                self.calculate_threshold_stats()\n",
    "\n",
    "        ## transform results lists to dataframe:        \n",
    "        new_column_names = list(feature_info.columns)[:-2] \n",
    "        new_column_names += ['AUC(MRC)', 'p(MRC)', 'threshold', 'specificity(EPE)', 'sensitivity(EPE)', 'EF_number_controls', 'EF_number_patients']\n",
    "        result_df = pd.DataFrame(data = self.stat_results, columns = new_column_names)\n",
    "        \n",
    "        return(result_df)\n",
    "    \n",
    "    \n",
    "    def extract_features(self, features, roi_shape, controls_file_paths, patients_file_paths):\n",
    "        \n",
    "        \"\"\"\n",
    "        print('extracting features)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## create list for store features :\n",
    "        \n",
    "        self.feat_val_list = []\n",
    "\n",
    "    \n",
    "        ##---------------- deside single feature analysis or bulk analysis will be done\n",
    "\n",
    "        # for single feature ---->             \n",
    "        if len(features) == 1 :            \n",
    "            # check_features_existence:            \n",
    "            if self.feature_exist(features):                \n",
    "                if self.follow_proccess == 'all':\n",
    "                    print(f\" features : {features} were read from existed file \")                \n",
    "                return(0)\n",
    "\n",
    "            control_data = self.single_calculation(features.iloc[0, :], controls_file_paths, roi_shape)\n",
    "            patient_data = self.single_calculation(features.iloc[0, :], patients_file_paths, roi_shape)            \n",
    "            control_data['situation'] = [self.control_id] * len(control_data)\n",
    "            patient_data['situation'] = [self.patient_id] * len(patient_data)           \n",
    "            feat_result = pd.concat([control_data, patient_data], ignore_index=True)          \n",
    "            self.save_extracted_features(feat_result, list(features.iloc[0, :-2].values))       \n",
    "            self.feat_val_list.append([features.iloc[0, :], feat_result])     \n",
    "            del feat_result, control_data, patient_data   ## clean objects\n",
    "\n",
    "        # for multipl features ---->\n",
    "        elif len(features) > 1 :\n",
    "            #check if features proccessed and saved before:           \n",
    "            if self.feature_exist(features) : \n",
    "                if self.follow_proccess == 'all':\n",
    "                    print(f\" features : {features} were read from existed files \")            \n",
    "                return(0)\n",
    "            # extract features\n",
    "            self.control_datas = self.multipl_calculation(features, controls_file_paths, roi_shape)\n",
    "            self.patient_datas = self.multipl_calculation(features, patients_file_paths, roi_shape)            \n",
    "            feat_num = -1 ## for double check features matching            \n",
    "            while self.patient_datas:                \n",
    "                feat_num += 1\n",
    "                feat0 = features.iloc[feat_num, :]\n",
    "                feat_control, control_data = self.control_datas[0]\n",
    "                feat_patient, patient_data = self.patient_datas[0]                \n",
    "                self.control_datas.pop(0), self.patient_datas.pop(0)  ## clean data\n",
    "                \n",
    "                ## checking if we are making counting for same features\n",
    "                if  (feat0.values == feat_control.values).all() and (feat_control.values == feat_patient.values).all() :\n",
    "                    \n",
    "                    control_data['situation'] = [self.control_id] * len(control_data)\n",
    "                    patient_data['situation'] = [self.patient_id] * len(patient_data)\n",
    "                    feat_result = pd.concat([control_data, patient_data], axis = 0, ignore_index=True)                    \n",
    "                    self.save_extracted_features(feat_result, list(features.iloc[feat_num, :-2].values))                                    \n",
    "                    ## store multipl features and values dataframes to list\n",
    "                    self.feat_val_list.append([feat0, feat_result])                     \n",
    "                    del feat_result ## clean object\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def multipl_calculation(self, features, file_paths, roi_shape):\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        all_feat_and_values = [] ## list for store values(dataframes) with their features(list)                        \n",
    "        ## create dataframes for storage selected features values       \n",
    "        for feat_num in range(len(features)):\n",
    "            all_feat_and_values.append([ features.iloc[feat_num, :], pd.DataFrame()])      \n",
    "        ## limiting calculation since we need to calculate untill max. power or step\n",
    "        pw_min, pw_max = features['power'].min(), features['power'].max()     \n",
    "\n",
    "        ## specificity sr list\n",
    "        s_r_list = list(features['secondary_rate'].unique())\n",
    "        s_r_list = [int(sr) for sr in s_r_list]   \n",
    "        ## specificity step size\n",
    "        step = features['step'].iloc[0]\n",
    "        for file_names in file_paths:      \n",
    "            if self.follow_proccess == 'all':\n",
    "                print(f\" '-- {file_names[0]} --'  is being processed \") \n",
    "            fe = Feature_Extraction(stripped = file_names[0], original = file_names[1], \n",
    "                        min_power = pw_min,  max_power = pw_max,\n",
    "                        step = step,\n",
    "                        ROI_shape = roi_shape, \n",
    "                        primary_rate = 'all', secondary_rate = s_r_list,\n",
    "                        all_powers = True, all_steps = False)   \n",
    "            \n",
    "            fe.calculate_features()         \n",
    "            eliminated_coordinates_numbers = fe.eliminated_coordinates_number ## get numbers to add coordinates\n",
    "            case_id = self.extract_case_id(file_names[0])\n",
    "            \n",
    "            for feat_num in range(len(features)):      \n",
    "                \n",
    "                ## specificity feature infos\n",
    "                feature_ = features.iloc[feat_num, :]\n",
    "               \n",
    "                ## get values from fe object\n",
    "                pw, p_r, s_r =  feature_['power'],  feature_['primary_rate'], feature_['secondary_rate'] \n",
    "                feature_values = fe.rates[str(s_r)][p_r-1][pw - pw_min]                \n",
    "                \n",
    "                ##create dataframe with values and coordinates\n",
    "                x,y,z = np.indices(feature_values.shape)  ## specificity indices for  coordinates\n",
    "\n",
    "\n",
    "                # create dataframe with values and coordinates\n",
    "                df = pd.DataFrame(np.c_[x.ravel(), y.ravel(), z.ravel(), feature_values.ravel()], columns=((['x','y','z', 'value'])))\n",
    "                df = df.dropna(how='any') ## drop nan values\n",
    "\n",
    "                ### convert coordinates to real values\n",
    "                df[['x', 'y', 'z']] = df[['x', 'y', 'z']] + np.tile(eliminated_coordinates_numbers, (len(df),1))\n",
    "\n",
    "                ## convert x-y-z columns to coordinates column by using tuple\n",
    "                df['coordinates'] = df[['x', 'y', 'z']].apply(tuple, axis=1)\n",
    "\n",
    "                ## remove x-y-z columns\n",
    "                df = df[['value', 'coordinates']]\n",
    "\n",
    "                ## add case id to dataframe\n",
    "                df['case'] = [case_id] * len(df)\n",
    "                \n",
    "                ## order dataframe depend on value and take only maximum PCVL ( per case value limit)\n",
    "                df.sort_values(by = 'value', axis=0, ascending=False, inplace=True,  ignore_index=True)\n",
    "                if self.PCVL < len(df) :\n",
    "                    df = df.iloc[:self.PCVL , :]\n",
    "                \n",
    "                ## unite dataframe with other cases dataframes\n",
    "                all_feat_and_values[feat_num][1] = pd.concat([df, all_feat_and_values[feat_num][1]], ignore_index=True)\n",
    " \n",
    "            del fe  ## clean object\n",
    "    \n",
    "        return(all_feat_and_values)\n",
    "    \n",
    "\n",
    "    def  single_calculation(self, feature_info, file_paths, roi_shape) :    \n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.follow_proccess == 'all':\n",
    "            print(f\" feature extraction being procced for following files :  \") \n",
    "\n",
    "        feat_vals = pd.DataFrame(columns = ['case', 'value']) \n",
    "        pw, step = feature_info['power'], feature_info['step']\n",
    "        p_r, s_r = feature_info['primary_rate'], feature_info['secondary_rate']\n",
    "        \n",
    "        for file_names in file_paths:                   \n",
    "            if self.follow_proccess == 'all':\n",
    "                print(f\" '-- {file_names[0]} --'  is being processed \") \n",
    "            fe = Feature_Extraction(stripped = file_names[0], original = file_names[1], \n",
    "                            power = pw, step = step, ROI_shape = roi_shape, \n",
    "                            primary_rate = p_r, secondary_rate = s_r)            \n",
    "            fe.calculate_features() ## calculate per features and MRC of features\n",
    "            eliminated_coordinates_numbers = fe.eliminated_coordinates_number ## get numbers to add coordinates\n",
    "            ## get feature array\n",
    "            feature_values = fe.rates[str(s_r)]\n",
    "            ##create dataframe with values and coordinates\n",
    "            x,y,z = np.indices(feature_values.shape)  ## specificity indices for  coordinates    \n",
    "\n",
    "            \n",
    "            # create dataframe with values and coordinates\n",
    "            df = pd.DataFrame(np.c_[x.ravel(), y.ravel(), z.ravel(), feature_values.ravel()], columns=((['x','y','z', 'value'])))            \n",
    "            df = df.dropna(how='any') ## drop nan values     \n",
    "            \n",
    "            ### convert coordinates to real values\n",
    "            df[['x', 'y', 'z']] = df[['x', 'y', 'z']] + np.tile(eliminated_coordinates_numbers, (len(df),1))\n",
    "        \n",
    "            ## convert x-y-z columns to coordinates column by using tuple\n",
    "            df['coordinates'] = df[['x', 'y', 'z']].apply(tuple, axis=1)  \n",
    "            \n",
    "            ## remove x-y-z columns\n",
    "            df = df[['value', 'coordinates']]           \n",
    "            ## specificity case id                 \n",
    "            case_id = self.extract_case_id(file_names[0])\n",
    "            ## add case id to dataframe\n",
    "            df['case'] = [case_id] * len(df)\n",
    "            ## order dataframe depend on value and take only maximum PCVL ( per case value limit)\n",
    "            df.sort_values(by = 'value', axis=0, ascending=False, inplace=True,  ignore_index=True)\n",
    "            if self.PCVL < len(df) :\n",
    "                df = df.iloc[:self.PCVL , :]\n",
    "            ## unite dataframe with other cases dataframes\n",
    "            feat_vals = pd.concat([feat_vals, df], ignore_index=True)\n",
    "            \n",
    "            del fe ## clean objects\n",
    "\n",
    "        return(feat_vals)\n",
    "    \n",
    "\n",
    "  \n",
    "   \n",
    "                \n",
    "\n",
    "        \n",
    "    def   calculate_threshold_stats(self):\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## check if collective analysis will be made\n",
    "        if self.activate_collective_analysis : \n",
    "            \n",
    "            coll_case_infos =  take_collective_case_infos([self.control_id, self.patient_id],\n",
    "                                                               self.info_path, self.sequence_folders)\n",
    "            \n",
    "            self.coll_control_info = coll_case_infos[coll_case_infos['situation'] == self.control_id]\n",
    "            self.coll_patient_info = coll_case_infos[coll_case_infos['situation'] == self.patient_id]\n",
    "        \n",
    "\n",
    "        ## calculating stats for per feature\n",
    "        \n",
    "        while self.feat_val_list:\n",
    "            \n",
    "            ## assign objects\n",
    "            self.feature, self.value_df = self.feat_val_list[0]\n",
    "            \n",
    "            ## clean unnecessary object\n",
    "            self.feat_val_list.pop(0)\n",
    "                         \n",
    "            ## update dataframe for collective analysis if necessary\n",
    "            if self.activate_collective_analysis : \n",
    "                self.activate_collective_analysis_()\n",
    "            \n",
    "            \n",
    "            ## define threshold list for values:           \n",
    "            self.values = self.value_df['value'].values\n",
    "            \n",
    "            thresold, specificity, sensitivity, EF_controls, EF_patients = self.find_cut_off()\n",
    "            \n",
    "\n",
    "            if specificity >= self.specificity_min:\n",
    "                \n",
    "                self.stat_results.append(list(self.feature.values) + [thresold, specificity, sensitivity, EF_controls, EF_patients])\n",
    "                \n",
    "            if self.follow_proccess != 'none' :\n",
    "                if self.follow_proccess == 'feature_and_stat':\n",
    "                    print(f\" feature parameters :  {self.feature}\")\n",
    "                if specificity >= self.specificity_min:\n",
    "                    print(f\" specificity score: {specificity}, sensitivity score: {sensitivity}\")\n",
    "                else:\n",
    "                    print('scores were not found above desired limits')\n",
    "\n",
    "            \n",
    "\n",
    "    def activate_collective_analysis_(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ## updating control case infos:\n",
    "  \n",
    "        for case_num in range(len(self.coll_control_info)):\n",
    "            row = self.coll_control_info.iloc[case_num, :]\n",
    "            case_id_list = [folder + '-' + row[folder] for folder in self.sequence_folders if type(row[folder]) == str ]            \n",
    "            for case_id in case_id_list:        \n",
    "                self.value_df['case'] = np.where((self.value_df['case'] == case_id)  & (self.value_df['situation'] == self.control_id), row['case'], self.value_df['case']  )\n",
    "     \n",
    "    \n",
    "        ## updating patients case infos :\n",
    "        \n",
    "        for case_num in range(len(self.coll_patient_info)):\n",
    "            row = self.coll_patient_info.iloc[case_num, :]\n",
    "            case_id_list = [folder + '-' + row[folder] for folder in self.sequence_folders if type(row[folder]) == str ]            \n",
    "            for case_id in case_id_list:        \n",
    "                self.value_df['case'] = np.where((self.value_df['case'] == case_id)  & (self.value_df['situation'] == self.patient_id), row['case'], self.value_df['case']  )\n",
    "     \n",
    "    \n",
    "        \n",
    "    def find_cut_off(self):    \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" \n",
    "\n",
    "        ## create threshold list with standart deviation step size\n",
    "        threshold_list = np.linspace(np.quantile(self.values, 0.9), np.nanmax(self.values), 1000)\n",
    "        \n",
    "        threshold_number = len(threshold_list)\n",
    "\n",
    "        ## create dataframe with same values columns\n",
    "        values_arr = np.tile(self.values.reshape(-1, 1), (1, threshold_number))\n",
    "\n",
    "        ## transform to dtaframe for making calculations           \n",
    "        thresholds_id = list(np.arange(threshold_number))\n",
    "        df = pd.DataFrame(data = values_arr, columns = thresholds_id)\n",
    "\n",
    "        ## filtering dataframe based on thresholds\n",
    "        df[df <= threshold_list] = 0\n",
    "        df[df > threshold_list] = 1\n",
    "\n",
    "        ## place case ids and situations to indicate positive results owners\n",
    "        df[['situation', 'case']] = self.value_df[['situation', 'case']]\n",
    "\n",
    "        ## seperate controls and patients\n",
    "        df_patients = df[thresholds_id + ['case']][df['situation'] == self.patient_id]\n",
    "        df_controls = df[thresholds_id + ['case']][df['situation'] == self.control_id]\n",
    "        \n",
    "        \n",
    "        ##  patients cases numbers\n",
    "        patient_num, control_num = df_patients.nunique()['case'], df_controls.nunique()['case']\n",
    "\n",
    "        ## specificity true positives and false positives\n",
    "        TP, FP = self.take_positive_cases_number(df_patients), self.take_positive_cases_number(df_controls)\n",
    "        TN, FN = control_num - FP, patient_num - TP\n",
    "\n",
    "        #np.divide(pw_std_all, self.std_ngb, out=np.zeros_like(pw_std_all), where=self.std_ngb!=0)\n",
    "        stat = pd.DataFrame()\n",
    "        stat['sensitivity'] = np.divide(TP.astype(float) , (TP+FN).astype(float) , out=np.zeros_like(TP.astype(float)), where=(TP+FN)!=0)\n",
    "        stat['specificity'] = np.divide(TN.astype(float) , (TN+FP).astype(float) , out=np.zeros_like(TN.astype(float)), where=(TN+FP)!=0)\n",
    "\n",
    "        ## Eliminate rows that do not correspond to the desired sensitivity limit\n",
    "        stat.loc[stat['sensitivity'] < self.sensitivity_min, 'specificity'] = 0 \n",
    "\n",
    "        maxValueIndex = stat['specificity'].idxmax()\n",
    "        thresold = threshold_list[maxValueIndex]\n",
    "        specificity, sensitivity = stat['specificity'].iloc[maxValueIndex], stat['sensitivity'].iloc[maxValueIndex]\n",
    "        EF_controls, EF_patients = df_controls[maxValueIndex].sum(), df_patients[maxValueIndex].sum()\n",
    "        \n",
    "        return(thresold, specificity, sensitivity, EF_controls, EF_patients)\n",
    "\n",
    "\n",
    "    \n",
    "    def take_positive_cases_number(self, data_df):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(data_df) == 0:\n",
    "            return(0)\n",
    "        \n",
    "        ## find positives\n",
    "        data_df = data_df.groupby(by = 'case').sum()\n",
    "        ## positive cases will be 1\n",
    "        data_df[data_df > 0] = 1\n",
    "        ## find number of positive cases per threshold:\n",
    "        positive_case_numbers = data_df.sum(axis = 0).values\n",
    "        \n",
    "        return(positive_case_numbers)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def extract_case_id(self, file_path):\n",
    "    \n",
    "        \"\"\"\n",
    "        This function gives case id of nifti file by extracting from file path\n",
    "\n",
    "        parameters:\n",
    "        file_path (type: string (path_like)) : nifti file path (file extension : .nii.gz)\n",
    "\n",
    "        outputs:\n",
    "        case_id (type : str) : case id\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.activate_collective_analysis:\n",
    "            splitted_file = os.path.split(file_path)\n",
    "            case_id = splitted_file[1].split('.nii.gz')[0]\n",
    "            folder_name = os.path.split(splitted_file[0])[1]\n",
    "            coll_case_id = folder_name + '-' + case_id\n",
    "            return(coll_case_id)\n",
    "\n",
    "        file_name = os.path.split(file_path)[1]\n",
    "        case_id = file_name.split('.nii.gz')[0]\n",
    "        return(case_id)\n",
    "    \n",
    "    \n",
    "    def save_extracted_features(self, data, features_list):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        features_list = [str(feat) for feat in features_list] + ['.pkl']\n",
    "        file_name = \"-\".join(features_list)\n",
    "\n",
    "        output_file_path = os.path.join(self.feature_output_folder, file_name)\n",
    "        data.to_pickle(output_file_path) \n",
    "    \n",
    "    \n",
    "    def feature_exist(self, features):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        file_exist = False\n",
    "        \n",
    "        ## check if single feature list given or multipl features were given\n",
    "        \n",
    "        if len(features) ==  1:\n",
    "            ## single feature were given:\n",
    "            file_path = self.take_pkl_file_path(features.iloc[0, :-2].values)\n",
    "            if os.path.isfile(file_path) :            \n",
    "                self.feat_val_list = [[features.iloc[0, :], pd.read_pickle(file_path)]]\n",
    "                file_exist = True                \n",
    "            return(file_exist)\n",
    "\n",
    "        elif len(features) > 1:            \n",
    "            ## multipl features we given \n",
    "            file_paths = list(map(self.take_pkl_file_path, features.iloc[:, :-2].values))             \n",
    "            not_exist_features = [i for i, file_path in enumerate(file_paths) if not os.path.isfile(file_path)]\n",
    "            \n",
    "            if not_exist_features:                \n",
    "                return(file_exist)\n",
    "                            \n",
    "            self.feat_val_list = [[features.iloc[i, :], pd.read_pickle(file_path)] for i, file_path in enumerate(file_paths) if os.path.isfile(file_path)]            \n",
    "            \n",
    "            if self.feat_val_list :                \n",
    "                file_exist = True\n",
    "\n",
    "            return(file_exist)\n",
    "        \n",
    "\n",
    "                 \n",
    "    def take_pkl_file_path(self, feature_vals):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        features_list = [str(feat) for feat in feature_vals] + ['.pkl']\n",
    "        file_name = \"-\".join(features_list)\n",
    "        file_path = os.path.join(self.feature_output_folder, file_name)\n",
    "        \n",
    "        return(file_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78618c1-0152-443f-ac90-c2eabd8fe6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
