{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5ecb72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbc12c74",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Contents:\n",
    "\n",
    "0 - import necessary libraries\n",
    "\n",
    "1 - general aim functions \n",
    "\n",
    "    1.1 - create_folder (function)\n",
    "         : creates new folders\n",
    "    1.2 - create_mirror_subfolders\n",
    "         : create mirror folder     \n",
    "    1.3 - take_folder_list (function)\n",
    "         : gives folder lists within spesific folder     \n",
    "    1.4 - take_path_list\n",
    "         : gives absolute paths lists  (function)\n",
    "    1.5 - take_spesific_file_names (function)\n",
    "         : gives file names list with spesific extension\n",
    "    1.6 - take_mutual_members (function)\n",
    "         : gives mutual members of lists within dictionary\n",
    "    1.7 - take_spesific_files_paths(functions\n",
    "         : gives a list of file path which are with the defined extension within given folder\n",
    "    1.8 - special_filter_path\n",
    "         : seperate and filter given path based on given conditions\n",
    "    1.9 - extract_case_id\n",
    "         : extract case id from nifti file path\n",
    "    \n",
    "2- function for obtain MRI acquisition informations\n",
    "\n",
    "    2.1 - take_acquisition_info  (function)\n",
    "      : this function gives chosen dicom metadata in data frame format\n",
    "\n",
    "3 -  class convert dicom file to nifti files\n",
    "\n",
    "    3.1 convert_dcm_folders_to_nifti_folder (class)    \n",
    "      : converts dicom file sequences to single nifti file (.nii.gz)\n",
    "\n",
    "4 - spesific functions to MRC analysis\n",
    "\n",
    "    4.1 - take_special_file_list (function)\n",
    "         : gives spesific file path and folder path informations for MRC analysis\n",
    "    4.2 - read_multipl_csv_files (function)\n",
    "         : gives a list which in consist of MRC results\n",
    "    4.3 - read_multipl_csv_files_to_dict (function)\n",
    "         : gives a list consist of dataframes which are including features informations and multipl MRC results\n",
    "\n",
    "5 - Functions and classes for statistical analysis\n",
    "\n",
    "    5.1 - analyse_MRC_results (class)\n",
    "         : makes Mann-Whitney U test and ROC Curve analysis and gives p values and AUC stats\n",
    "    5.2 - find_p_value (function)\n",
    "         : gives p value based on Mann-Whitney U test         \n",
    "    5.3 - find_auc (function)\n",
    "         : gives AUC based on ROC analysis\n",
    "    5.4 - save_stat_to_csv(function)\n",
    "         : save given data into the csv file in a specific format\n",
    "\n",
    "      \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181bb39",
   "metadata": {},
   "source": [
    "# 0 - import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45db2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import pathlib\n",
    "\n",
    "import dicom2nifti\n",
    "import pydicom\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "import sklearn.metrics as metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4665a85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.Feature_extraction_classes_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced57b86",
   "metadata": {},
   "source": [
    "# 1- general aim functions or classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071117c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1\n",
    "def create_folder(folder):\n",
    "    \"\"\"\n",
    "    --this function create folder \n",
    "    --you could give spesific folder path, or \n",
    "    if you give string the function will create a folder within same folder\n",
    "    \n",
    "    param  : folder (type : str)\n",
    "            - path_like string: directly the folder path\n",
    "            - folder_name : folder will be created in the same diroctory with the code\n",
    "            \n",
    "    output :  folder_path (type : str) \n",
    "                created folder path\n",
    "    \n",
    "    \"\"\"\n",
    "    folder_path = folder\n",
    "    \n",
    "    if os.path.exists(folder_path):\n",
    "        return(folder_path)\n",
    "    \n",
    "    if type(folder_path) is str :        \n",
    "        folder_path =  os.path.abspath(folder_path)\n",
    "        \n",
    "    os.mkdir(folder_path)\n",
    "    return(folder_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772e2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2\n",
    "def create_mirror_subfolders(folder_paths = [''], \n",
    "                         old_root_path = '', new_root_path = ''):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function creates new folders in the same hierarchy \n",
    "    in the given new root same as the given original root\n",
    "    \n",
    "    params  : \n",
    "    folder_paths (type : list) : existed folder paths for mirroring       \n",
    "    old_root_path (type : str(path_like)) : rooth path for beginning to the create mirror brunches\n",
    "    new_root_path (type : str(path_like)) : mirror brunches rooth path\n",
    "\n",
    "            \n",
    "    output : -\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for folder_path in folder_paths:\n",
    "        child_path = folder_path.split(old_root_path)[-1]\n",
    "        new_path = os.path.join(new_root_path, child_path[1:])\n",
    "        \n",
    "        if not os.path.exists(new_path):\n",
    "            os.mkdir(new_path)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3\n",
    "def take_folder_list(folder_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives a list of folder names within given folderpath\n",
    "    \n",
    "    params  : \n",
    "    folder_path (type : str(path_like)) : existed folder path  \n",
    "            \n",
    "    output: \n",
    "    folder_list (type : list) : folder names list\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(folder_path):\n",
    "        return(print('\"take_folder_names_list\" function error : \\n the given path is not a valid folder path'))\n",
    "    \n",
    "    folder_list = os.listdir(folder_path)\n",
    "    folder_list = [folder for folder in folder_list if  os.path.isdir(os.path.join(folder_path, folder))]\n",
    "    \n",
    "    return(folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4\n",
    "def take_path_list(root_folder, folder_list):\n",
    "\n",
    "    \"\"\"\n",
    "    this function gives a list of folders' paths withing given folderpath\n",
    "    \n",
    "    params  : \n",
    "    root_folder (type : str(path_like))  : existed folder path \n",
    "    folder_list (type : list) : folder names' list within root_folder\n",
    "            \n",
    "    output: \n",
    "    folder_list (type : list) : folder paths' list\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    root_path = os.path.abspath(root_folder)\n",
    "    if not os.path.exists(root_path):\n",
    "        return(print('\"take_path_list\" function error : \\n the given root_folder is not a valid folder'))\n",
    "    if type(folder_list) is not list:\n",
    "        return(print('\"take_path_list\" function error :\\n the input is not a list'))\n",
    "    if not folder_list:\n",
    "        return(print('\"take_path_list\" function error :\\n the input list is empty'))\n",
    "    \n",
    "    paths = []\n",
    "    \n",
    "    for folder in folder_list:\n",
    "        paths.append(os.path.join(root_path, folder))\n",
    "        \n",
    "    return (paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5\n",
    "\n",
    "def take_spesific_file_names(path, extension):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives a list of dcm file names withing given folder\n",
    "    \n",
    "    params  : \n",
    "    path (type : str(path_like))  : folder path which includes files\n",
    "    extension  (type : str)  : spesific extention of file for listing\n",
    "\n",
    "    output: \n",
    "    dcm_list (type : list) : dicom files' names' list\n",
    "    \n",
    "    \"\"\"\n",
    "    file_list = os.listdir(path)\n",
    "    file_list = [file for file in file_list if extension in file]\n",
    "    return(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3089e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6\n",
    "\n",
    "def take_mutual_members(member_dict):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives a list of member which are exist in each members of dictionary\n",
    "    \n",
    "    params  : \n",
    "    member_dict (type : dictionary)  : the dictionary includes lists which are including members(int/float/str)\n",
    "\n",
    "\n",
    "    output: \n",
    "    mutual_members (type : list) : this list includes mutual members of lists of dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if type(member_dict) is dict :\n",
    "        \n",
    "        # specify dictionary keys\n",
    "        dict_keys = list(member_dict.keys())\n",
    "        \n",
    "        # get all members of first key\n",
    "        members0 = member_dict[dict_keys[0]]\n",
    "        \n",
    "        # compare first key members if they are also member of other keys\n",
    "        mutual_members = [member for member in members0 for key in dict_keys[1:] if member in member_dict[key]]\n",
    "        \n",
    "        return(mutual_members)\n",
    "    \n",
    "    else:\n",
    "        return(print('the input is not a dictionary'))\n",
    "        \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.7\n",
    "def take_spesific_files_paths(root_folder = '', extension = '.csv'):\n",
    "    \n",
    "    \"\"\"\n",
    "    gives a list of file path which are with the defined extension within given folder\n",
    "    \n",
    "    params  : \n",
    "    root_folder (type : str(path_like))  : indicates folder path \n",
    "    extension (type: str)  : point out files type for taking spesific files\n",
    "\n",
    "    output: \n",
    "    file_paths (type : list) : this list includes files paths with the given extension\n",
    "    \n",
    "    \"\"\"\n",
    "    # take all files\n",
    "    file_names = os.listdir(root_folder)\n",
    "    # take files only with the spesific extension\n",
    "    file_names = [file for file in file_names if extension in file]\n",
    "    # define files paths\n",
    "    file_paths = [os.path.join(root_folder, file_name) for file_name in file_names]\n",
    "    \n",
    "    return(file_paths)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d02d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.8 \n",
    "def special_filter_path(path_list, seperate_conditions = [], include_features = []):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    this function filter gives a specific path list based on given separate conditions and features\n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    path_list (type: list) : list which consists of lists and these lists includes file paths\n",
    "    seperate_conditions (type: list) : list contains conditions for sperating paths \n",
    "    include_features (type: list) : includes strings which are expecting to be inside of paths\n",
    "    \n",
    "    outputs:\n",
    "    path_dic (type:dict) : includes selected file paths lists\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    path_dic = {}\n",
    "    \n",
    "    for condition in seperate_conditions:\n",
    "        \n",
    "        path_dic[condition] = [path for path in path_list if any(feat in path[0] for feat in include_features) and (condition in path[0])]\n",
    "                               \n",
    "    return(path_dic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29017523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.9 \n",
    "def extract_case_id(file_path, collective_analysis = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    change\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file_name = os.path.split(file_path)[1]\n",
    "    case_id = file_name.split('.nii.gz')[0]\n",
    "    \n",
    "    if collective_analysis:\n",
    "        folder = os.path.dirname(file_path).split('/')[-1]\n",
    "        case_id = folder + '-' + case_id\n",
    "    \n",
    "    return(case_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b1337",
   "metadata": {},
   "source": [
    "# 2- function for obtain MRI acquisition informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d07b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.1\n",
    "def take_acquisition_info(data_path, case, case_id):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives chosen dicom metadata in data frame format\n",
    "    \n",
    "    params  : \n",
    "    data_path (type : str(path_like))  : dicom file path for obtain acquisition informations\n",
    "    case (type : str) : case name inside original folders\n",
    "    case_id (type : str) : assigned case id which will be used in further process\n",
    "            \n",
    "    output: \n",
    "    df (type : pandas.DataFrame) : dataframe contains cquisition information type as column name\n",
    "                                    and value is the corresponding value in metadata\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    dicom_files = take_spesific_file_names(data_path, '.dcm')\n",
    "    \n",
    "    if not dicom_files:\n",
    "        return('folder is not including dicom data')\n",
    "    \n",
    "    dicom_path = os.path.join(data_path, dicom_files[0])\n",
    "    dataset = pydicom.dcmread(dicom_path)\n",
    "\n",
    "    data_informations = {}\n",
    "\n",
    "    data_informations['input_name'] = case\n",
    "    data_informations['case_name'] = case_id\n",
    "    data_informations['TR'] = dataset[0x0018, 0x0080].value\n",
    "    data_informations['TE'] = dataset[0x0018, 0x0081].value\n",
    "    data_informations['FA'] = dataset[0x0018, 0x1314].value\n",
    "    data_informations['Percent_Phase_FOV'] = dataset[(0x0018, 0x0094)].value\n",
    "    data_informations['Spacing_Between_Slices'] = dataset[0x0018, 0x0088].value\n",
    "    data_informations['Slice_Thickness'] = dataset[0x0018, 0x0050].value\n",
    "    data_informations['Rows'] = dataset[0x0028, 0x0010].value\n",
    "    data_informations['Columns'] = dataset[0x0028, 0x0011].value\n",
    "    data_informations['Pixel_Spacing'] = dataset[0x0028, 0x0030].value\n",
    "    data_informations['Acquisition_Matrix'] = str(dataset[0x0018, 0x1310].value)\n",
    "    data_informations['MR_Acquisition_Type'] = str(dataset[0x0018, 0x0023].value)\n",
    "    data_informations['Manufacturer'] = str(dataset[0x0008, 0x0070].value)\n",
    "    data_informations['Magnetic_Field_Strength'] = str(dataset[0x0018, 0x0087].value)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data_informations, orient='columns')\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### this function will be used in next code for checking input and output files corresponding identity\n",
    "\n",
    "def check_if_files_corresponding(path1, path2):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function check the children and first parents of given files for saving stripped data correctly\n",
    "    params:\n",
    "    path1 (type : str) : input file path  (will be stripped)\n",
    "    path2 (type : str) : output file path (stripped file will be saved as this path)\n",
    "    output : \n",
    "    files_corresponding(type: bool) : True: if files are matching correctly\n",
    "                                      False: if files are not matching    \n",
    "    \"\"\"\n",
    "    \n",
    "    files_corresponding = True\n",
    "    \n",
    "    path1, path2 = pathlib.PurePath(path1), pathlib.PurePath(path2)\n",
    "    \n",
    "    if path1.name != path2.name:\n",
    "        files_corresponding = False\n",
    "        \n",
    "    if path1.parent.name != path2.parent.name:\n",
    "        files_corresponding = False\n",
    "    \n",
    "    return(files_corresponding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33add577",
   "metadata": {},
   "source": [
    "# 3 -  class convert dicom file to nifti files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451fb03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.1\n",
    "\n",
    "class convert_dcm_folders_to_nifti_folder():\n",
    "    \n",
    "    \"\"\"\n",
    "    this class find each dicom folder and transform to nifti files within given folder hierarchy\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        \n",
    "    def take_folder_list(self, folder_path):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        function gives folder names' list within given folder path\n",
    "        \n",
    "        params  : \n",
    "        folder_path (type : str(path_like))  : folder path\n",
    "\n",
    "        output: \n",
    "        folder_list (type : list) : folder names lists within given folder (folder_path)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # creating subfolder list within given folder\n",
    "        folder_list = os.listdir(folder_path)\n",
    "        folder_list = [folder for folder in folder_list if  os.path.isdir(os.path.join(folder_path, folder))]\n",
    "        return(folder_list)\n",
    "    \n",
    "    def transform_files(self, origin_folder, target_folder):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        this function take an original folder and enter inside each subfolder;\n",
    "        if subfolder consists dicom series then transform to nifti format and save to spesific path \n",
    "        which is mirror folder of the transform dicom series\n",
    "        is fubfolder does not include dicom series, the function continue to enter subfolders\n",
    "        until reading all subfolders\n",
    "        \n",
    "        params  : \n",
    "        origin_folder (type : str(path_like))  : folder path which includes Dicom files series\n",
    "        target_folder (type : str(path_like))  : folder path for saving nifti which transformed from dicom series\n",
    "        \n",
    "\n",
    "        output: -\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        folder_paths_list = self.take_folder_list(origin_folder)   \n",
    "        \n",
    "        ## take new folder names\n",
    "        for folder in folder_paths_list:      \n",
    "            \n",
    "            ## define data_path\n",
    "            data_path = os.path.join(origin_folder, folder)\n",
    "            target_path = os.path.join(target_folder, folder)\n",
    "\n",
    "            ## check inside of folder if it has still subfolder\n",
    "            new_folder_list = self.take_folder_list(data_path)\n",
    "        \n",
    "            ## if it has subfolder create same subfolder inside nifti brunch\n",
    "            if new_folder_list:\n",
    "                \n",
    "                if not os.path.exists(target_path):\n",
    "                    create_folder(target_path)\n",
    "                    \n",
    "                self.transform_files(data_path, target_path)\n",
    "\n",
    "            ## if there is no subfolder anymore, convert data inside of folder to the nifti and save\n",
    "            else:\n",
    "                create_folder(target_path)\n",
    "                \n",
    "                dicom2nifti.convert_directory(data_path, target_path)\n",
    "                \n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb86e42",
   "metadata": {},
   "source": [
    "# 4 - spesific functions to MRC analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5057c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1\n",
    "def take_special_file_list(mainfolders, root_path, original_root_path):\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    this function gives spesific file path and folder path informations for MRC analysis\n",
    "    \n",
    "    params  : \n",
    "    mainfolders (type : list)  : list which is including main folder e.g. : ['patients', 'controls']\n",
    "    root_path (type : str(path_like)) : stripped folders' root path \n",
    "    original_root_path (type : str(path_like)) :  original nifti files (not stripped) root folder paths \n",
    "            \n",
    "    output: \n",
    "    file_names_list (type : list) : list members include : [stripped nifti files path, original nifti file path]\n",
    "    folder_path_list (type : list): includes subfolder paths for saving MRC results\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    file_names_list = []\n",
    "    folder_path_list = []\n",
    "    \n",
    "    for mainfolder in mainfolders:\n",
    "        \n",
    "        ## record main folder path\n",
    "        folder_path_list.append(os.path.join(root_path, mainfolder))\n",
    "\n",
    "        ## create subfolder list\n",
    "        subfolders = take_folder_list(os.path.join(root_path, mainfolder))\n",
    "\n",
    "        for subfolder in subfolders:\n",
    "\n",
    "            ## specify subfolder path\n",
    "            subfolder_path = os.path.join(root_path, mainfolder, subfolder)\n",
    "            \n",
    "            ## record subfolder path\n",
    "            folder_path_list.append(subfolder_path)\n",
    "\n",
    "            ## taking case names lists\n",
    "            case_list = os.listdir(os.path.join(root_path, mainfolder, subfolder))\n",
    "            case_list = [case  for case in case_list if '.nii.gz' in case]\n",
    "            \n",
    "            for case in case_list:\n",
    "                ## saving stripped and original file paths\n",
    "                file_names_list.append([os.path.join(subfolder_path, case),\n",
    "                                        os.path.join(original_root_path, mainfolder, subfolder, case)])\n",
    "                \n",
    "                \n",
    "    return (file_names_list, folder_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e186d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2\n",
    "def read_multipl_csv_files(file_path_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives a list contains given csv files data\n",
    "    \n",
    "    parameter:\n",
    "    \n",
    "    file_path_list(type: list) : contains csv file paths\n",
    "    \n",
    "    output:\n",
    "    \n",
    "    csv_results(type: list) : include data results of csv files\n",
    "    \n",
    "    \"\"\"\n",
    "    csv_results = []\n",
    "    for csv_file in file_path_list:\n",
    "        data = pd.read_csv(csv_file)\n",
    "        csv_results.append(data)\n",
    "    return(csv_results)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3\n",
    "def read_multipl_csv_files_to_dict(path_dict, cases_info):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives a list consist of dataframes which are including \n",
    "        features informations and MRC results from different subfolders belong to same sequences\n",
    "        \n",
    "    path_dict(type: dict) : the dictionary includes file paths whose keys are corresponding to their subfolders\n",
    "    cases_info (type: pandas.dataframe) : the dataframe includes each case' case_ids in different subfolders\n",
    "    \n",
    "    output: mutual_MRC(type: list) : the list contains of dictionaries which are including\n",
    "                            feature informations and MRC results belong to each different subfolder from same sequences\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # create a list for store MRC dataframes\n",
    "    mutual_MRC = []    \n",
    "    # get subfolder list for creating dataframe with MRC results from each subfolder\n",
    "    subfolders = list(path_dict.keys())\n",
    "    \n",
    "    for case_num in range(len(cases_info)):\n",
    "        \n",
    "        case_ids = cases_info.iloc[case_num, 1:]\n",
    "        \n",
    "        ## taking 1 subfolder results for creating template dataframe\n",
    "        for i, subfolder in enumerate(subfolders):            \n",
    "            template_case = case_ids[subfolder]                        \n",
    "            if type(template_case) == str :                 \n",
    "                template_path = [path for path in path_dict[subfolder] if template_case in path][0]\n",
    "                df = pd.read_csv(template_path)\n",
    "\n",
    "                \n",
    "                ## record which subfolder number were taken as a template\n",
    "                template_folder = subfolder\n",
    "                template_num = i\n",
    "                ## end searching for template when we found a valid case_id\n",
    "                break\n",
    "        \n",
    "\n",
    "        # update dataframe MRC results column name\n",
    "        df = df.rename(columns = {'MRC_value': template_folder })        \n",
    "        \n",
    "        ## unite dataframe with remained subfolders\n",
    "        for subfolder in (subfolders[template_num + 1 :]):            \n",
    "            case_id = case_ids[subfolder]            \n",
    "            if type(case_id) == str :             \n",
    "                case_path = [path for path in path_dict[subfolder] if case_id in path][0]\n",
    "                case_df = pd.read_csv(case_path) \n",
    "                \n",
    "                ## add MRC_value column to dataframe\n",
    "                df[subfolder] = case_df['MRC_value']\n",
    "\n",
    "        mutual_MRC.append(df)  \n",
    "    \n",
    "    return(mutual_MRC)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e38c2a",
   "metadata": {},
   "source": [
    "# 5 - Functions and classes for statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb102b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 \n",
    "\n",
    "class analyse_MRC_results():\n",
    "    \n",
    "    \"\"\"\n",
    "    1. This class takes file paths, analyse data and makes Mann-Whitney U test and ROC Curve Analysis\n",
    "       , gives p values and AUC results with given features\n",
    "    2. class makes also correlation analysis for features\n",
    "        which are found statistically meaningful based on given stat limits \n",
    "    \n",
    "    parameters:\n",
    "    \n",
    "    control_csv_paths(type: list)(**) : control cases files csv paths\n",
    "    patients_csv_paths(type: list)(**) : patients files csv paths\n",
    "    ** :these params obligatory for single folder analysis, should not given if the analysis is collective\n",
    "    collective_evaluation (type: bool) :\n",
    "            True: collective analysis will be done \n",
    "            False: single folders analysis will be done\n",
    "            \n",
    "    NOTE: if  : collective_evaluation = True:\n",
    "        you need to call \"activate_collective_evaluation\" function before taking stat results\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, control_csv_paths = [], patients_csv_paths = [], \n",
    "                 collective_evaluation = False):\n",
    "        \n",
    "        \n",
    "        if not collective_evaluation :\n",
    "            \n",
    "            ## checking given inputs if they are lists\n",
    "            self.check_if_list(control_csv_paths, 'control_csv_paths')\n",
    "            self.check_if_list(patients_csv_paths, 'patients_csv_paths')\n",
    "\n",
    "            ## assign list paths\n",
    "            self.control_paths = control_csv_paths\n",
    "            self.patient_paths = patients_csv_paths\n",
    "            \n",
    "            ## directly load data\n",
    "            self.contols_data = read_multipl_csv_files(self.control_paths)\n",
    "            self.patients_data = read_multipl_csv_files(self.patient_paths)\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "    def activate_collective_evaluation(self, control_csv_paths = {}, patients_csv_paths = {},  \n",
    "                                       sequence_folders = [], caseid_info_file='.xlsx',\n",
    "                                       control_id = '', patient_id = ''):\n",
    "        \"\"\"\n",
    "        this function activates collective analysis, \n",
    "        compare MRC results from different subfolders which are belong to same sequence \n",
    "        and create dataframes which are includes maximum MRC results of sequence folders\n",
    "        \n",
    "        parameters:\n",
    "        \n",
    "        control_csv_paths(type: dict) : dictionary consists of control cases file paths \n",
    "                                    whose keys are corresponding to their subfolders\n",
    "        \n",
    "        patients_csv_paths(type: dict) : dictionary consists of patients file paths \n",
    "                                    whose keys are corresponding to their subfolders\n",
    "                                    \n",
    "        sequence_folders (type: list) : includes subfolder names of sequence\n",
    "        \n",
    "        caseid_info_file(type: str(path_like)) : gives excel file path for clarify cases ids in subgroups\n",
    "        \n",
    "        control_id(type: str) : the word which represent coltrol groups main folder name\n",
    "        patient_id(type: str) : the word which represent patient groups main folder name\n",
    "        \n",
    "        output: -- \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        ## get cases subfolder info info \n",
    "        control_info = self.create_case_id_data(caseid_info_file, sequence_folders, control_id)        \n",
    "        patient_info = self.create_case_id_data(caseid_info_file, sequence_folders, patient_id)\n",
    "\n",
    "        # make dataframes which are includes different MRC results from same sequences MRI types\n",
    "        collective_contols_data  = read_multipl_csv_files_to_dict(control_csv_paths, control_info)\n",
    "        collective_patients_data = read_multipl_csv_files_to_dict(patients_csv_paths, patient_info)\n",
    "        \n",
    "        # get dataframes which includes maximum MRC results for statistical analysis\n",
    "        self.contols_data = self.get_max_MRC(collective_contols_data)\n",
    "        self.patients_data = self.get_max_MRC(collective_patients_data)\n",
    "        \n",
    "    \n",
    "    def get_max_MRC(self, df_list):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function calculate maximum MRC results among different subfolders MRC results\n",
    "        \n",
    "        params:\n",
    "        \n",
    "        df_list (type: list) : contains dataframes which are including feature informations \n",
    "                and MRC results from different subfolders\n",
    "                \n",
    "        outputs:\n",
    "        \n",
    "        data (type: list) : contains dataframes which are including feature informations \n",
    "                and maximum MRC results of different subfolders\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # specify list for record each cases dataframe results\n",
    "        \n",
    "        data = []\n",
    "\n",
    "        for df in df_list:            \n",
    "            # create new df which is including only features info\n",
    "            new_df = df.iloc[:, :5].copy()\n",
    "            # add to dataframe maximums of MRC results \n",
    "            new_df['MRC_value'] = df[list(df.columns)[5:]].max(axis = 1, skipna = True)\n",
    "            \n",
    "            data.append(new_df)\n",
    "\n",
    "        return(data)\n",
    "    \n",
    "\n",
    "    def take_stats(self, folder_type = '', auc_limit = 0.75, p_limit = 0.01):\n",
    "        \n",
    "        \n",
    "        \"\"\"    \n",
    "        This function process the Mann Whitney U test and ROC Curve analysis and give p values and AUC results. \n",
    "        Additionally, it creates an object that includes features with \n",
    "        only meaningful stats based on defined limits.\n",
    "        Additionaly in this function, meaningful features values are stored for correlation analysis \n",
    "        \n",
    "        params: \n",
    "        folder_type (type: str) : subfolder or sequence for discriminate stat results origin\n",
    "        auc_limit (type: float) : limit for accepting AUC result as meaningful\n",
    "        p_limit (type: float) : limit for accepting p values as meaningful\n",
    "        \n",
    "        outputs :\n",
    "        p_results (type: list) : gives features with their p values\n",
    "        AUC_results (type: list) : gives features with their AUC results\n",
    "        meaningful_results (type: list) : gives meaningful features with their AUC and p results\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        ## create object for store feature values to making correlation analysis\n",
    "        self.values_df = pd.DataFrame()\n",
    "        empty_df = True\n",
    "        \n",
    "        template_data = self.contols_data[0]\n",
    "        \n",
    "        feature_number = len(template_data)\n",
    "        \n",
    "        p_results, AUC_results, self.meaningful_results = [], [], []\n",
    "        \n",
    "\n",
    "        for feature_num in range(feature_number):\n",
    "            \n",
    "            feature_info = [folder_type] + list(template_data.iloc[feature_num, :-1].values)\n",
    "            \n",
    "            control_values = []\n",
    "            patient_values = []\n",
    "            \n",
    "            for control_data in self.contols_data:\n",
    "                control_values.append(control_data.iloc[feature_num, -1])\n",
    "                \n",
    "            for patient_data in self.patients_data:\n",
    "                patient_values.append(patient_data.iloc[feature_num, -1])\n",
    "             \n",
    "            ## get p value(Mann-Whitney U test)\n",
    "            p_value = round(find_p_value(control_values, patient_values), 4)\n",
    "            ## record p value with feature info\n",
    "            p_results.append(feature_info + [p_value])\n",
    "            \n",
    "            ## get AUC value(ROC curve)\n",
    "            auc_value = round(find_auc(control_values, patient_values), 2)\n",
    "            ## record AUC value with feature info\n",
    "            AUC_results.append(feature_info + [auc_value])\n",
    "            \n",
    "            ## record meaningful stat results based on given limits\n",
    "            if (auc_value > auc_limit) and (p_value < p_limit):\n",
    "                self.meaningful_results.append(feature_info + [auc_value] + [p_value])\n",
    "                \n",
    "                # store values for corr.analysis and make column names as feature index\n",
    "                temp_df = pd.DataFrame()\n",
    "                temp_df[str(len(self.meaningful_results) - 1)] = control_values + patient_values\n",
    "                self.values_df = pd.concat([self.values_df, temp_df], axis = 1, join='outer')\n",
    "\n",
    "            \n",
    "        return(p_results, AUC_results, self.meaningful_results)\n",
    "    \n",
    "    \n",
    "    def power_analysis(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        this function search parameters features and for features who who has same parameters for \n",
    "        \" 'MRI_type', 'shape', 'primary_rate', 'secondary_rate', 'step' \" but different for power,\n",
    "        select the feature with highest AUC value and drop others.\n",
    "        \n",
    "        parameters:\n",
    "        \n",
    "        -- use self.meaningful results without calling withing function \n",
    "            which is list of all meaningful features\n",
    "            \n",
    "        outputs:\n",
    "        \n",
    "        df (type: list) : The list of remaining features after\n",
    "                        dropping the feature has the same params but different power\n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        ## checking if list is not empty\n",
    "        if self.meaningful_results:\n",
    "        \n",
    "            ## create dataframe from meaningful result and sort based on AUC values\n",
    "            feature_column = ['MRI_type', 'shape', 'primary_rate', 'secondary_rate', 'step', 'power', 'AUC', 'p']\n",
    "            df = pd.DataFrame(data = self.meaningful_results, columns = feature_column)\n",
    "\n",
    "            ## sorting df based on AUC values\n",
    "            df = df.sort_values(by = 'AUC', ascending = False)\n",
    "\n",
    "            ## dropping if all features same but power, keeping the rows with highes AUC value\n",
    "            df.drop_duplicates(subset=df.columns[:5], keep='first',  inplace=True, ignore_index=True)\n",
    "\n",
    "            ## convert dataframe to list \n",
    "            df = df.values.tolist()\n",
    "            \n",
    "            return(df)\n",
    "        \n",
    "        else:\n",
    "            return([])\n",
    "        \n",
    "    \n",
    "        \n",
    "            \n",
    "    def create_case_id_data(self, info_path, sequence_folders, situation):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function reads an excel file and find per case case_ids in each subfolder and give as a dataframe\n",
    "        \n",
    "        params:\n",
    "        \n",
    "        info_path (type: str (path_like)) : excel file path which includes case and case ids info\n",
    "        sequence_folders (type: list) : includes subfolders names which are belong to same sequence\n",
    "        situation(type: str) : define if the desired dataframe will be created for patient or control group\n",
    "        \n",
    "        outputs:\n",
    "        \n",
    "        info_df (type: dict) : consists of case names and their ids within different subfolders\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        ## reading info file\n",
    "        file_ = pd.read_excel(info_path, sheet_name = None)\n",
    "        ## taking sheet names\n",
    "        sheet_names = list(file_.keys())\n",
    "        ## filtering sheet names based on situation\n",
    "        sheet_names = [sheet for sheet in sheet_names if situation in sheet]\n",
    "        ## filtering sheet names depend on subfolders which are belong to same sequences\n",
    "        sheet_names = [sheet for sheet in sheet_names for subfolder in sequence_folders if subfolder in sheet]\n",
    "        \n",
    "        ##create dataframe for loading infos\n",
    "        info_df = pd.DataFrame(columns = ['input_name'])\n",
    "\n",
    "        for sheet in sheet_names :\n",
    "            \n",
    "            df = file_[sheet][['input_name', 'case_name']]\n",
    "            new_column_name = sheet.split(situation + '_')[1]\n",
    "            df = df.rename(columns = {'case_name': new_column_name })\n",
    "            info_df = pd.merge(info_df, df, how = \"outer\", on=['input_name'])\n",
    "\n",
    "        return (info_df)\n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "    def check_if_list(self, list_, list_name):\n",
    "        \n",
    "        \"\"\"\n",
    "        this function check if input is a list , \n",
    "        if not, the function will be stop running code \n",
    "        and gives a message with object name for correct inputs\n",
    "        \n",
    "        params:\n",
    "        \n",
    "        list_(type: -) : this will be check if it is a list or not\n",
    "        list_name (type: str) : this is an object name will be in error message if input (list_) is not a list\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if type(list_) is not list:\n",
    "            print(f' Error Message : {list_name} input is not a list')\n",
    "            exit()\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453752f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 \n",
    "\n",
    "def find_p_value (list1, list2):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives p value of Mann-Whitney U test between given 2 list members\n",
    "    \n",
    "    params  : \n",
    "    list1 (type : list)  : list includes numeric values\n",
    "    list2 (type : list)  : list includes numeric values\n",
    "\n",
    "    output: \n",
    "    p (type : float) : this value has a mean for statistical analysis \n",
    "    \"\"\"\n",
    "    \n",
    "    U1, p = mannwhitneyu(list1, list2, method=\"exact\")\n",
    "    \n",
    "    return(p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42632a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3 \n",
    "\n",
    "def find_auc(controls_values = [], patient_values = []):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function gives AUC values which is area under ROC curve\n",
    "    \n",
    "    params  : \n",
    "    controls_values (type : list)  : this list consists controls MRC values\n",
    "                         \n",
    "    patient_values (type : list)  : this list consists patients MRC values\n",
    "\n",
    "    output: \n",
    "    roc_auc (type : float) : area under ROC curve\n",
    "                            this value has a mean for statistical analysis\n",
    "    \n",
    "    \"\"\"\n",
    "    situation_labels = [0] * len(controls_values) + [1] * len(patient_values)\n",
    "    values = controls_values + patient_values\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(situation_labels, values)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    return(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3ea534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4\n",
    "\n",
    "def save_stat_to_csv(data, labels = [], path = '.csv', sort_columns = False,\n",
    "                     ascending_order = True, sorting_columns = []):\n",
    "    \"\"\"\n",
    "    this function create dataframe with given data and column names (labels) and save to csv file\n",
    "    \n",
    "    data (type: list) : includes features and stat results\n",
    "    labels(type : list) : includes column names of data\n",
    "    path (type: str (path_like)) : specify path to save csv file\n",
    "    ascending_order (type: bool) : specify if we should order dataframe in ascending order or not based on \"sorting_columns\"\n",
    "    sorting_columns (type : list) : specify column names for ordering data\n",
    "    \n",
    "    output: -\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.DataFrame(data, columns = labels)\n",
    "    if sort_columns:\n",
    "        df = df.sort_values(by=sorting_columns, ascending=ascending_order) ## reorder rows based on better stat results\n",
    "    df.to_csv(path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
